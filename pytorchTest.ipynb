{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBMuEcuqym/Epf/mOcNY4Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashrotd/AIClassifierModel/blob/main/pytorchTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ5Zu39nrUQX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "qTR6hJ5zuHIT"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('fashion-mnist_train.csv')"
      ],
      "metadata": {
        "id": "woBImv_yuPHE"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "GYevkMEeudNS",
        "outputId": "ccb89704-5b55-4326-8757-b90f0017eccb"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
              "0      2       0       0       0       0       0       0       0       0   \n",
              "1      9       0       0       0       0       0       0       0       0   \n",
              "2      6       0       0       0       0       0       0       0       5   \n",
              "3      0       0       0       0       1       2       0       0       0   \n",
              "4      3       0       0       0       0       0       0       0       0   \n",
              "\n",
              "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
              "0       0  ...         0         0         0         0         0         0   \n",
              "1       0  ...         0         0         0         0         0         0   \n",
              "2       0  ...         0         0         0        30        43         0   \n",
              "3       0  ...         3         0         0         0         0         1   \n",
              "4       0  ...         0         0         0         0         0         0   \n",
              "\n",
              "   pixel781  pixel782  pixel783  pixel784  \n",
              "0         0         0         0         0  \n",
              "1         0         0         0         0  \n",
              "2         0         0         0         0  \n",
              "3         0         0         0         0  \n",
              "4         0         0         0         0  \n",
              "\n",
              "[5 rows x 785 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f8d8f3a1-2aa8-4252-b700-6af1d3e7d143\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 785 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8d8f3a1-2aa8-4252-b700-6af1d3e7d143')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f8d8f3a1-2aa8-4252-b700-6af1d3e7d143 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f8d8f3a1-2aa8-4252-b700-6af1d3e7d143');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = df_train.drop(['label'],axis=1)"
      ],
      "metadata": {
        "id": "9wZ3hE6KufJZ"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.head()\n",
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfQ0TRm0uwch",
        "outputId": "1c9ac668-f9f3-4113-c638-b474e13c51c6"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = df_train['label']\n",
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZJubwxIuxbR",
        "outputId": "5975bc55-3223-4c2f-c3c6-3525ab984f69"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJOrRGWbv3_L",
        "outputId": "d049070f-beb6-4041-ba66-d8e7d116493c"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2\n",
              "1    9\n",
              "2    6\n",
              "3    0\n",
              "4    3\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "lCg-7HtHxidL",
        "outputId": "e3391d6e-0c42-491c-d5cb-7d96bc58d7f3"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
              "0       0       0       0       0       0       0       0       0       0   \n",
              "1       0       0       0       0       0       0       0       0       0   \n",
              "2       0       0       0       0       0       0       0       5       0   \n",
              "3       0       0       0       1       2       0       0       0       0   \n",
              "4       0       0       0       0       0       0       0       0       0   \n",
              "\n",
              "   pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
              "0        0  ...         0         0         0         0         0         0   \n",
              "1        0  ...         0         0         0         0         0         0   \n",
              "2        0  ...         0         0         0        30        43         0   \n",
              "3        0  ...         3         0         0         0         0         1   \n",
              "4        0  ...         0         0         0         0         0         0   \n",
              "\n",
              "   pixel781  pixel782  pixel783  pixel784  \n",
              "0         0         0         0         0  \n",
              "1         0         0         0         0  \n",
              "2         0         0         0         0  \n",
              "3         0         0         0         0  \n",
              "4         0         0         0         0  \n",
              "\n",
              "[5 rows x 784 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-73b48f4a-f530-4681-a12e-ad73c9b6ef18\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 784 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73b48f4a-f530-4681-a12e-ad73c9b6ef18')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-73b48f4a-f530-4681-a12e-ad73c9b6ef18 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-73b48f4a-f530-4681-a12e-ad73c9b6ef18');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(x_train)"
      ],
      "metadata": {
        "id": "52VP4fGrxkxb"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2cSZjlrzptA",
        "outputId": "821398ab-dcda-4fda-c3a1-ca663a80fa90"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784,)"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = x_train[123].reshape(28,28)"
      ],
      "metadata": {
        "id": "iVnobVbP0KyX"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Label :',{y_train[123]})\n",
        "plt.imshow(a)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "mDmWERFUxn5K",
        "outputId": "00172a5e-c72c-4fab-fb42-f9564fa3e2e8"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label : {6}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train/255.0"
      ],
      "metadata": {
        "id": "WBhWwqoCxpzz"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlJJGGR05q1S",
        "outputId": "b2dfab82-6e33-4a0a-985c-bbadc9eed9d0"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.argmax()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK57Bsgj5wPN",
        "outputId": "e1416d00-499e-4c4b-953f-568deff5afa7"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "123"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY9Dgchk54Wr",
        "outputId": "c33d7628-8143-4463-c752-0d3f5ac902a2"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784,)"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.rand(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ3S4XQa60mz",
        "outputId": "2ab343a7-a0e4-47eb-f68a-db03f86c076b"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5185, 0.7630, 0.5604])"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.empty(5,2,3)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwJjPjGE900F",
        "outputId": "59271ba3-782a-4d2a-f09e-829b90656236"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.3452e-43, 0.0000e+00, 6.7262e-44],\n",
              "         [0.0000e+00, 6.7681e-34, 0.0000e+00]],\n",
              "\n",
              "        [[4.2039e-45, 0.0000e+00, 3.3631e-44],\n",
              "         [0.0000e+00, 1.1210e-44, 0.0000e+00]],\n",
              "\n",
              "        [[6.7262e-44, 0.0000e+00, 1.3593e-43],\n",
              "         [0.0000e+00, 4.8938e-34, 0.0000e+00]],\n",
              "\n",
              "        [[5.5498e-29, 0.0000e+00, 0.0000e+00],\n",
              "         [1.8750e+00, 0.0000e+00, 0.0000e+00]],\n",
              "\n",
              "        [[6.7262e-44, 0.0000e+00, 6.7262e-44],\n",
              "         [0.0000e+00, 5.5498e-29, 0.0000e+00]]])"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.ones(2,3,dtype=torch.int)\n",
        "x.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYuI7PhgI7J-",
        "outputId": "d4ed2071-eee9-4297-8a25-7ff8fac331d8"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int32"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.rand(2,2)\n",
        "y=torch.rand(2,2)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbKUARLWKTsc",
        "outputId": "46a5e89f-1cfd-48d7-d71e-7f9ba2d203bc"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7973, 0.4177],\n",
              "        [0.5039, 0.7097]])"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z=torch.div(x,y)\n",
        "z\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3jUv8_dKmXe",
        "outputId": "e232612c-847d-44f1-c106-b6df65a8cd3f"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3261, 0.1894],\n",
              "        [0.2335, 0.2388]])"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.rand(5,3)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVoNF_V6KyMw",
        "outputId": "085c9ecb-b286-4469-8f70-af2dbfd2cbee"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0910, 0.3841, 0.2925],\n",
              "        [0.1746, 0.3982, 0.7382],\n",
              "        [0.7658, 0.3931, 0.1696],\n",
              "        [0.8134, 0.3029, 0.2915],\n",
              "        [0.8212, 0.3711, 0.9591]])"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iOgKgEDfLZuQ"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6eXJ2SGLeyn"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=x.view(15)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYWsfz-XLfeI",
        "outputId": "2c135d7f-f44f-4980-ffc1-09e6e2f092c7"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0910, 0.3841, 0.2925, 0.1746, 0.3982, 0.7382, 0.7658, 0.3931, 0.1696,\n",
              "        0.8134, 0.3029, 0.2915, 0.8212, 0.3711, 0.9591])"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x= y.view(3,5)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgDkI41JM4ta",
        "outputId": "e5475bd2-6ca1-42c5-fe1b-80d41d7dab51"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0910, 0.3841, 0.2925, 0.1746, 0.3982],\n",
              "        [0.7382, 0.7658, 0.3931, 0.1696, 0.8134],\n",
              "        [0.3029, 0.2915, 0.8212, 0.3711, 0.9591]])"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.ones(5)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PmJlVL4NF_e",
        "outputId": "098c57be-6af4-4afa-8047-80a58c113fb7"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b=a.numpy()"
      ],
      "metadata": {
        "id": "0yLM-A7YNW1D"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B67Egc27NY34",
        "outputId": "5cdf62d5-166e-493b-f646-e3940fb0d2ec"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.add_(1)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWs7LVZdNZ4L",
        "outputId": "3fc8efe6-eb3a-48a4-d418-891e4b906fa8"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 2., 2., 2., 2.])"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAB7QBC0NtD3",
        "outputId": "c4ad9a59-285f-4d01-8274-9e25f5831e1e"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2., 2., 2., 2., 2.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3,requires_grad=True)"
      ],
      "metadata": {
        "id": "agzk5T_CNvJn"
      },
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=x+2\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxyjmxWeWtm9",
        "outputId": "bf8581af-2bac-4d91-9d2e-2f5023028fd5"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.0639, 2.0461, 1.9347], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.ones(4,requires_grad=True)\n",
        "print(weights)\n",
        "for epoch in range(3):\n",
        "  model_output = (weights*3).mean()\n",
        "  print(model_output)\n",
        "  model_output.backward()\n",
        "  print(weights.grad)\n",
        "  weights.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yntRWY55Wvi4",
        "outputId": "d5371d6b-71fe-42b5-9e3b-b2a5f8d836ae"
      },
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.], requires_grad=True)\n",
            "tensor(3., grad_fn=<MeanBackward0>)\n",
            "tensor([0.7500, 0.7500, 0.7500, 0.7500])\n",
            "tensor(3., grad_fn=<MeanBackward0>)\n",
            "tensor([0.7500, 0.7500, 0.7500, 0.7500])\n",
            "tensor(3., grad_fn=<MeanBackward0>)\n",
            "tensor([0.7500, 0.7500, 0.7500, 0.7500])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(weights,lr=0.01)\n",
        "optimizer.step()\n",
        "optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "gakQds9jXszW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BackProp\n",
        "x = torch.tensor(1.0)"
      ],
      "metadata": {
        "id": "J0wKi_EDd79b"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_dwbmZLilSi",
        "outputId": "46088c9e-c499-4311-ac55-1e5da1401276"
      },
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=torch.tensor(2.0)"
      ],
      "metadata": {
        "id": "_g61NUn_ilia"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.tensor(1.0,requires_grad=True)"
      ],
      "metadata": {
        "id": "lYOfaEcain0J"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#forward pass\n",
        "y_hat = w*x\n",
        "loss = (y_hat-y)**2\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BjK1MOQiuVX",
        "outputId": "f4ebdcb7-a5a4-4064-b80b-7a09631471c1"
      },
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#backward Pass\n",
        "loss.backward()\n",
        "w.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCMfOTJOi5KJ",
        "outputId": "b0cb1fd4-e3e1-428b-bd48-f9d63eb80bc6"
      },
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-2.)"
            ]
          },
          "metadata": {},
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1.0)\n",
        "w = torch.tensor(1.0,requires_grad=True)\n",
        "y=torch.tensor(2.0)"
      ],
      "metadata": {
        "id": "xyab-gw2i_9c"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = x*w\n",
        "loss = (y_hat-y)**2\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkshc2Z8jhO4",
        "outputId": "4ac47fbb-cce6-45ca-b934-ef723c867e36"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "w.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-fd4j2Hj0a5",
        "outputId": "2c3896ad-385b-4c10-e698-578aaaf7d5ac"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-2.)"
            ]
          },
          "metadata": {},
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(10.0)\n",
        "w = torch.tensor(3.0,requires_grad=True)\n",
        "y = torch.tensor(2.0)\n",
        "z = w*x\n",
        "loss = (z-y)**2\n",
        "loss.backward()\n",
        "w.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9upbryKLj7LG",
        "outputId": "1b3615f3-d00f-4cff-f16d-b9591da64625"
      },
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(560.)"
            ]
          },
          "metadata": {},
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "9mac90sNk2o3"
      },
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#f = w*x\n",
        "#f=2*x\n",
        "X = np.array([1,2,3,4], dtype=np.float32)\n",
        "Y = np.array([2,4,6,8],dtype=np.float32)\n",
        "w=0"
      ],
      "metadata": {
        "id": "tpJbWwR8njlh"
      },
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model prediction\n",
        "def forward(x):\n",
        "  return w*x"
      ],
      "metadata": {
        "id": "X37qquJEn7jT"
      },
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y,y_pred):\n",
        "  return ((y_pred-y)**2).mean()"
      ],
      "metadata": {
        "id": "hOKM5g13oBXX"
      },
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gradient of loss\n",
        "#mse = 1/N * (w*x-y)**2 \n",
        "def gradient(x,y,y_pred):\n",
        "  return np.dot(2*x,y_pred-y).mean()\n",
        "  "
      ],
      "metadata": {
        "id": "A4sr1W4joNqk"
      },
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bDapbcrpcVo",
        "outputId": "cef130a2-3c9a-4262-fbee-3870a618b5c8"
      },
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before Training: F(5)=0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training params\n",
        "learning_rate = 0.01\n",
        "n_iters = 10"
      ],
      "metadata": {
        "id": "fia4ykmspuCS"
      },
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epochs in range(n_iters):\n",
        "  #predict forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  #loss\n",
        "  l = loss(Y,y_pred)\n",
        "\n",
        "  #gradients\n",
        "  dw = gradient(X,Y,y_pred)\n",
        "\n",
        "  #updata weights\n",
        "  w -= learning_rate*dw\n",
        "\n",
        "  if epochs % 1 ==0:\n",
        "    print(f'Epoch {epochs+1}: w={w:.3f},loss={l:.8f}')\n",
        "print(f'Prediction before Training: F(5)={forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrRf8MEgp5F7",
        "outputId": "2d5276ff-3b1e-438c-ad83-05cab8bc7549"
      },
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: w=2.000,loss=0.00000033\n",
            "Epoch 2: w=2.000,loss=0.00000005\n",
            "Epoch 3: w=2.000,loss=0.00000001\n",
            "Epoch 4: w=2.000,loss=0.00000000\n",
            "Epoch 5: w=2.000,loss=0.00000000\n",
            "Epoch 6: w=2.000,loss=0.00000000\n",
            "Epoch 7: w=2.000,loss=0.00000000\n",
            "Epoch 8: w=2.000,loss=0.00000000\n",
            "Epoch 9: w=2.000,loss=0.00000000\n",
            "Epoch 10: w=2.000,loss=0.00000000\n",
            "Prediction before Training: F(5)=10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([1,2,3,4,5], dtype=np.float32)\n",
        "Y = np.array([3,6,9,12,15], dtype=np.float32)\n",
        "w = 0"
      ],
      "metadata": {
        "id": "axDSkjdkqyyM"
      },
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x):\n",
        "  return x*w"
      ],
      "metadata": {
        "id": "cIH1_nAzrbE5"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(y,y_pred,x):\n",
        "  return np.dot(2*x,y_pred-y).mean()"
      ],
      "metadata": {
        "id": "T4YUi3SSrg0K"
      },
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y,y_pred):\n",
        "  return ((y_pred-y)**2).mean()"
      ],
      "metadata": {
        "id": "NsqbCLhEtDVx"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.01\n",
        "n_itr = 10\n"
      ],
      "metadata": {
        "id": "veQLMb2VtTfr"
      },
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epochs in range(n_itr):\n",
        "  y_pred = forward(X)\n",
        "  l = loss(Y,y_pred)\n",
        "  dw = gradient(Y,y_pred,X)\n",
        "  w = w-lr*dw\n",
        "\n",
        "  if(epochs%1==0):\n",
        "     print(f'Epoch {epochs+1}: w={w:.3f},loss={l:.8f}')\n",
        "   # print(f'Number of Epochs: {epochs+1}predicted value = {y_pred:.3f},loss={l:.8f}')\n",
        "print(f'Predicted value of F(10): {forward(10)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDESXOgatbej",
        "outputId": "e2fdaaf4-653c-4d7e-cf0a-78544e0d30af"
      },
      "execution_count": 366,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: w=0.120,loss=30.00000000\n",
            "Epoch 2: w=0.233,loss=26.50799942\n",
            "Epoch 3: w=0.339,loss=23.42246819\n",
            "Epoch 4: w=0.439,loss=20.69609451\n",
            "Epoch 5: w=0.532,loss=18.28706932\n",
            "Epoch 6: w=0.620,loss=16.15845299\n",
            "Epoch 7: w=0.703,loss=14.27760887\n",
            "Epoch 8: w=0.781,loss=12.61569405\n",
            "Epoch 9: w=0.854,loss=11.14722919\n",
            "Epoch 10: w=0.923,loss=9.84969139\n",
            "Epoch 11: w=0.987,loss=8.70318699\n",
            "Epoch 12: w=1.048,loss=7.69013596\n",
            "Epoch 13: w=1.105,loss=6.79500437\n",
            "Epoch 14: w=1.159,loss=6.00406647\n",
            "Epoch 15: w=1.209,loss=5.30519199\n",
            "Epoch 16: w=1.257,loss=4.68766880\n",
            "Epoch 17: w=1.301,loss=4.14202356\n",
            "Epoch 18: w=1.343,loss=3.65989161\n",
            "Epoch 19: w=1.383,loss=3.23388100\n",
            "Epoch 20: w=1.420,loss=2.85745692\n",
            "Epoch 21: w=1.455,loss=2.52484894\n",
            "Epoch 22: w=1.487,loss=2.23095703\n",
            "Epoch 23: w=1.518,loss=1.97127318\n",
            "Epoch 24: w=1.547,loss=1.74181700\n",
            "Epoch 25: w=1.574,loss=1.53906977\n",
            "Epoch 26: w=1.600,loss=1.35992146\n",
            "Epoch 27: w=1.624,loss=1.20162702\n",
            "Epoch 28: w=1.646,loss=1.06175745\n",
            "Epoch 29: w=1.668,loss=0.93816900\n",
            "Epoch 30: w=1.687,loss=0.82896620\n",
            "Epoch 31: w=1.706,loss=0.73247433\n",
            "Epoch 32: w=1.724,loss=0.64721453\n",
            "Epoch 33: w=1.740,loss=0.57187837\n",
            "Epoch 34: w=1.756,loss=0.50531185\n",
            "Epoch 35: w=1.771,loss=0.44649369\n",
            "Epoch 36: w=1.784,loss=0.39452174\n",
            "Epoch 37: w=1.797,loss=0.34859943\n",
            "Epoch 38: w=1.810,loss=0.30802238\n",
            "Epoch 39: w=1.821,loss=0.27216870\n",
            "Epoch 40: w=1.832,loss=0.24048814\n",
            "Epoch 41: w=1.842,loss=0.21249539\n",
            "Epoch 42: w=1.851,loss=0.18776089\n",
            "Epoch 43: w=1.860,loss=0.16590551\n",
            "Epoch 44: w=1.869,loss=0.14659418\n",
            "Epoch 45: w=1.876,loss=0.12953064\n",
            "Epoch 46: w=1.884,loss=0.11445323\n",
            "Epoch 47: w=1.891,loss=0.10113093\n",
            "Epoch 48: w=1.897,loss=0.08935927\n",
            "Epoch 49: w=1.904,loss=0.07895778\n",
            "Epoch 50: w=1.909,loss=0.06976704\n",
            "Epoch 51: w=1.915,loss=0.06164616\n",
            "Epoch 52: w=1.920,loss=0.05447062\n",
            "Epoch 53: w=1.925,loss=0.04813030\n",
            "Epoch 54: w=1.929,loss=0.04252793\n",
            "Epoch 55: w=1.933,loss=0.03757768\n",
            "Epoch 56: w=1.937,loss=0.03320356\n",
            "Epoch 57: w=1.941,loss=0.02933867\n",
            "Epoch 58: w=1.945,loss=0.02592361\n",
            "Epoch 59: w=1.948,loss=0.02290622\n",
            "Epoch 60: w=1.951,loss=0.02023989\n",
            "Epoch 61: w=1.954,loss=0.01788393\n",
            "Epoch 62: w=1.957,loss=0.01580230\n",
            "Epoch 63: w=1.959,loss=0.01396293\n",
            "Epoch 64: w=1.962,loss=0.01233763\n",
            "Epoch 65: w=1.964,loss=0.01090154\n",
            "Epoch 66: w=1.966,loss=0.00963260\n",
            "Epoch 67: w=1.968,loss=0.00851134\n",
            "Epoch 68: w=1.970,loss=0.00752059\n",
            "Epoch 69: w=1.972,loss=0.00664521\n",
            "Epoch 70: w=1.974,loss=0.00587174\n",
            "Epoch 71: w=1.975,loss=0.00518827\n",
            "Epoch 72: w=1.977,loss=0.00458435\n",
            "Epoch 73: w=1.978,loss=0.00405072\n",
            "Epoch 74: w=1.979,loss=0.00357921\n",
            "Epoch 75: w=1.981,loss=0.00316260\n",
            "Epoch 76: w=1.982,loss=0.00279446\n",
            "Epoch 77: w=1.983,loss=0.00246920\n",
            "Epoch 78: w=1.984,loss=0.00218177\n",
            "Epoch 79: w=1.985,loss=0.00192782\n",
            "Epoch 80: w=1.986,loss=0.00170342\n",
            "Epoch 81: w=1.987,loss=0.00150514\n",
            "Epoch 82: w=1.987,loss=0.00132995\n",
            "Epoch 83: w=1.988,loss=0.00117512\n",
            "Epoch 84: w=1.989,loss=0.00103834\n",
            "Epoch 85: w=1.990,loss=0.00091749\n",
            "Epoch 86: w=1.990,loss=0.00081068\n",
            "Epoch 87: w=1.991,loss=0.00071632\n",
            "Epoch 88: w=1.991,loss=0.00063295\n",
            "Epoch 89: w=1.992,loss=0.00055927\n",
            "Epoch 90: w=1.992,loss=0.00049417\n",
            "Epoch 91: w=1.993,loss=0.00043665\n",
            "Epoch 92: w=1.993,loss=0.00038583\n",
            "Epoch 93: w=1.994,loss=0.00034092\n",
            "Epoch 94: w=1.994,loss=0.00030123\n",
            "Epoch 95: w=1.994,loss=0.00026617\n",
            "Epoch 96: w=1.995,loss=0.00023519\n",
            "Epoch 97: w=1.995,loss=0.00020781\n",
            "Epoch 98: w=1.995,loss=0.00018362\n",
            "Epoch 99: w=1.996,loss=0.00016225\n",
            "Epoch 100: w=1.996,loss=0.00014337\n",
            "Predicted value of F(10): 19.958902523517622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([1,2,3,4],dtype=torch.float32)\n",
        "Y = torch.tensor([5,6,7,8],dtype = torch.float32)\n",
        "w = torch.tensor(0.0,dtype=torch.float32,requires_grad=True)"
      ],
      "metadata": {
        "id": "BXwPznUkuWVw"
      },
      "execution_count": 428,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(x):\n",
        "  return x*w"
      ],
      "metadata": {
        "id": "3PafeMDhyUnP"
      },
      "execution_count": 429,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y,y_pred):\n",
        "  return ((y_pred-y)**2).mean()"
      ],
      "metadata": {
        "id": "kKJlq99pydzq"
      },
      "execution_count": 430,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_rmse(y,y_pred):\n",
        "  return (((y_pred-y)**2)**0.5).mean()"
      ],
      "metadata": {
        "id": "B3qUrtBZ4xwH"
      },
      "execution_count": 431,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vdY_qliLypXr"
      },
      "execution_count": 431,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.01\n",
        "n_itr = 1000"
      ],
      "metadata": {
        "id": "oXiGsGNdy0bh"
      },
      "execution_count": 432,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epochs in range(n_itr):\n",
        "  y_pred = forward_pass(X)\n",
        "  l = loss_rmse(Y,y_pred)\n",
        "  l.backward()\n",
        "  \n",
        " #w = w-lr*dw\n",
        "  with torch.no_grad():\n",
        "    w -= lr*w.grad\n",
        "\n",
        "  w.grad.zero_()\n",
        "  if(epochs%1==0):\n",
        "     #print(f'Epoch {epochs+1}: w={w:.3f},loss={l:.8f}')\n",
        "     print(f'Number of Epochs {epochs+1}:weights = {w:.3f},loss={l:.8f}')\n",
        "print(f'Predicted value of F(10): {forward(10)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHKzMRg7y3cZ",
        "outputId": "7e9a7285-cf9a-4155-d7ea-5cfea5cb9aa4"
      },
      "execution_count": 433,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epochs 1:weights = 0.025,loss=6.50000000\n",
            "Number of Epochs 2:weights = 0.050,loss=6.43749952\n",
            "Number of Epochs 3:weights = 0.075,loss=6.37500000\n",
            "Number of Epochs 4:weights = 0.100,loss=6.31250000\n",
            "Number of Epochs 5:weights = 0.125,loss=6.25000048\n",
            "Number of Epochs 6:weights = 0.150,loss=6.18750000\n",
            "Number of Epochs 7:weights = 0.175,loss=6.12499952\n",
            "Number of Epochs 8:weights = 0.200,loss=6.06250000\n",
            "Number of Epochs 9:weights = 0.225,loss=6.00000000\n",
            "Number of Epochs 10:weights = 0.250,loss=5.93750048\n",
            "Number of Epochs 11:weights = 0.275,loss=5.87500000\n",
            "Number of Epochs 12:weights = 0.300,loss=5.81249952\n",
            "Number of Epochs 13:weights = 0.325,loss=5.75000000\n",
            "Number of Epochs 14:weights = 0.350,loss=5.68750000\n",
            "Number of Epochs 15:weights = 0.375,loss=5.62500000\n",
            "Number of Epochs 16:weights = 0.400,loss=5.56250000\n",
            "Number of Epochs 17:weights = 0.425,loss=5.50000000\n",
            "Number of Epochs 18:weights = 0.450,loss=5.43750000\n",
            "Number of Epochs 19:weights = 0.475,loss=5.37500000\n",
            "Number of Epochs 20:weights = 0.500,loss=5.31250000\n",
            "Number of Epochs 21:weights = 0.525,loss=5.25000000\n",
            "Number of Epochs 22:weights = 0.550,loss=5.18750000\n",
            "Number of Epochs 23:weights = 0.575,loss=5.12500000\n",
            "Number of Epochs 24:weights = 0.600,loss=5.06250000\n",
            "Number of Epochs 25:weights = 0.625,loss=5.00000000\n",
            "Number of Epochs 26:weights = 0.650,loss=4.93750000\n",
            "Number of Epochs 27:weights = 0.675,loss=4.87500000\n",
            "Number of Epochs 28:weights = 0.700,loss=4.81250000\n",
            "Number of Epochs 29:weights = 0.725,loss=4.75000048\n",
            "Number of Epochs 30:weights = 0.750,loss=4.68750048\n",
            "Number of Epochs 31:weights = 0.775,loss=4.62500000\n",
            "Number of Epochs 32:weights = 0.800,loss=4.56250095\n",
            "Number of Epochs 33:weights = 0.825,loss=4.50000048\n",
            "Number of Epochs 34:weights = 0.850,loss=4.43750048\n",
            "Number of Epochs 35:weights = 0.875,loss=4.37500095\n",
            "Number of Epochs 36:weights = 0.900,loss=4.31250095\n",
            "Number of Epochs 37:weights = 0.925,loss=4.25000095\n",
            "Number of Epochs 38:weights = 0.950,loss=4.18750095\n",
            "Number of Epochs 39:weights = 0.975,loss=4.12500095\n",
            "Number of Epochs 40:weights = 1.000,loss=4.06250095\n",
            "Number of Epochs 41:weights = 1.025,loss=4.00000095\n",
            "Number of Epochs 42:weights = 1.050,loss=3.93750095\n",
            "Number of Epochs 43:weights = 1.075,loss=3.87500095\n",
            "Number of Epochs 44:weights = 1.100,loss=3.81250119\n",
            "Number of Epochs 45:weights = 1.125,loss=3.75000143\n",
            "Number of Epochs 46:weights = 1.150,loss=3.68750119\n",
            "Number of Epochs 47:weights = 1.175,loss=3.62500095\n",
            "Number of Epochs 48:weights = 1.200,loss=3.56250119\n",
            "Number of Epochs 49:weights = 1.225,loss=3.50000143\n",
            "Number of Epochs 50:weights = 1.250,loss=3.43750143\n",
            "Number of Epochs 51:weights = 1.275,loss=3.37500143\n",
            "Number of Epochs 52:weights = 1.300,loss=3.31250167\n",
            "Number of Epochs 53:weights = 1.325,loss=3.25000191\n",
            "Number of Epochs 54:weights = 1.350,loss=3.18750167\n",
            "Number of Epochs 55:weights = 1.375,loss=3.12500143\n",
            "Number of Epochs 56:weights = 1.400,loss=3.06250167\n",
            "Number of Epochs 57:weights = 1.425,loss=3.00000191\n",
            "Number of Epochs 58:weights = 1.450,loss=2.93750191\n",
            "Number of Epochs 59:weights = 1.475,loss=2.87500191\n",
            "Number of Epochs 60:weights = 1.500,loss=2.81250215\n",
            "Number of Epochs 61:weights = 1.525,loss=2.75000238\n",
            "Number of Epochs 62:weights = 1.550,loss=2.68750215\n",
            "Number of Epochs 63:weights = 1.575,loss=2.62500191\n",
            "Number of Epochs 64:weights = 1.600,loss=2.56250215\n",
            "Number of Epochs 65:weights = 1.625,loss=2.50000238\n",
            "Number of Epochs 66:weights = 1.650,loss=2.43750238\n",
            "Number of Epochs 67:weights = 1.675,loss=2.37500238\n",
            "Number of Epochs 68:weights = 1.700,loss=2.31250262\n",
            "Number of Epochs 69:weights = 1.725,loss=2.25000262\n",
            "Number of Epochs 70:weights = 1.750,loss=2.18750262\n",
            "Number of Epochs 71:weights = 1.775,loss=2.12500262\n",
            "Number of Epochs 72:weights = 1.800,loss=2.06250262\n",
            "Number of Epochs 73:weights = 1.825,loss=2.00000286\n",
            "Number of Epochs 74:weights = 1.850,loss=1.93750286\n",
            "Number of Epochs 75:weights = 1.875,loss=1.87500298\n",
            "Number of Epochs 76:weights = 1.900,loss=1.81250310\n",
            "Number of Epochs 77:weights = 1.925,loss=1.75000310\n",
            "Number of Epochs 78:weights = 1.950,loss=1.68750310\n",
            "Number of Epochs 79:weights = 1.975,loss=1.62500310\n",
            "Number of Epochs 80:weights = 2.000,loss=1.56250310\n",
            "Number of Epochs 81:weights = 2.025,loss=1.50000322\n",
            "Number of Epochs 82:weights = 2.030,loss=1.48750067\n",
            "Number of Epochs 83:weights = 2.035,loss=1.48500061\n",
            "Number of Epochs 84:weights = 2.040,loss=1.48250055\n",
            "Number of Epochs 85:weights = 2.045,loss=1.48000050\n",
            "Number of Epochs 86:weights = 2.050,loss=1.47750044\n",
            "Number of Epochs 87:weights = 2.055,loss=1.47500038\n",
            "Number of Epochs 88:weights = 2.060,loss=1.47250032\n",
            "Number of Epochs 89:weights = 2.065,loss=1.47000027\n",
            "Number of Epochs 90:weights = 2.070,loss=1.46750021\n",
            "Number of Epochs 91:weights = 2.075,loss=1.46500015\n",
            "Number of Epochs 92:weights = 2.080,loss=1.46250010\n",
            "Number of Epochs 93:weights = 2.085,loss=1.46000004\n",
            "Number of Epochs 94:weights = 2.090,loss=1.45749998\n",
            "Number of Epochs 95:weights = 2.095,loss=1.45499992\n",
            "Number of Epochs 96:weights = 2.100,loss=1.45249987\n",
            "Number of Epochs 97:weights = 2.105,loss=1.44999981\n",
            "Number of Epochs 98:weights = 2.110,loss=1.44749975\n",
            "Number of Epochs 99:weights = 2.115,loss=1.44499969\n",
            "Number of Epochs 100:weights = 2.120,loss=1.44249964\n",
            "Number of Epochs 101:weights = 2.125,loss=1.43999958\n",
            "Number of Epochs 102:weights = 2.130,loss=1.43749952\n",
            "Number of Epochs 103:weights = 2.135,loss=1.43499947\n",
            "Number of Epochs 104:weights = 2.140,loss=1.43249941\n",
            "Number of Epochs 105:weights = 2.145,loss=1.42999935\n",
            "Number of Epochs 106:weights = 2.150,loss=1.42749929\n",
            "Number of Epochs 107:weights = 2.155,loss=1.42499924\n",
            "Number of Epochs 108:weights = 2.160,loss=1.42249918\n",
            "Number of Epochs 109:weights = 2.165,loss=1.41999912\n",
            "Number of Epochs 110:weights = 2.170,loss=1.41749907\n",
            "Number of Epochs 111:weights = 2.175,loss=1.41499901\n",
            "Number of Epochs 112:weights = 2.180,loss=1.41249895\n",
            "Number of Epochs 113:weights = 2.185,loss=1.40999889\n",
            "Number of Epochs 114:weights = 2.190,loss=1.40749884\n",
            "Number of Epochs 115:weights = 2.195,loss=1.40499878\n",
            "Number of Epochs 116:weights = 2.200,loss=1.40249872\n",
            "Number of Epochs 117:weights = 2.205,loss=1.39999866\n",
            "Number of Epochs 118:weights = 2.210,loss=1.39749861\n",
            "Number of Epochs 119:weights = 2.215,loss=1.39499855\n",
            "Number of Epochs 120:weights = 2.220,loss=1.39249849\n",
            "Number of Epochs 121:weights = 2.225,loss=1.38999844\n",
            "Number of Epochs 122:weights = 2.230,loss=1.38749838\n",
            "Number of Epochs 123:weights = 2.235,loss=1.38499832\n",
            "Number of Epochs 124:weights = 2.240,loss=1.38249826\n",
            "Number of Epochs 125:weights = 2.245,loss=1.37999821\n",
            "Number of Epochs 126:weights = 2.250,loss=1.37749815\n",
            "Number of Epochs 127:weights = 2.255,loss=1.37499809\n",
            "Number of Epochs 128:weights = 2.260,loss=1.37249804\n",
            "Number of Epochs 129:weights = 2.265,loss=1.36999798\n",
            "Number of Epochs 130:weights = 2.270,loss=1.36749792\n",
            "Number of Epochs 131:weights = 2.275,loss=1.36499786\n",
            "Number of Epochs 132:weights = 2.280,loss=1.36249781\n",
            "Number of Epochs 133:weights = 2.285,loss=1.35999775\n",
            "Number of Epochs 134:weights = 2.290,loss=1.35749769\n",
            "Number of Epochs 135:weights = 2.295,loss=1.35499763\n",
            "Number of Epochs 136:weights = 2.300,loss=1.35249758\n",
            "Number of Epochs 137:weights = 2.305,loss=1.34999752\n",
            "Number of Epochs 138:weights = 2.310,loss=1.34749746\n",
            "Number of Epochs 139:weights = 2.315,loss=1.34499741\n",
            "Number of Epochs 140:weights = 2.320,loss=1.34249735\n",
            "Number of Epochs 141:weights = 2.325,loss=1.33999729\n",
            "Number of Epochs 142:weights = 2.330,loss=1.33749723\n",
            "Number of Epochs 143:weights = 2.335,loss=1.33499718\n",
            "Number of Epochs 144:weights = 2.325,loss=1.33500576\n",
            "Number of Epochs 145:weights = 2.330,loss=1.33749700\n",
            "Number of Epochs 146:weights = 2.335,loss=1.33499694\n",
            "Number of Epochs 147:weights = 2.325,loss=1.33500600\n",
            "Number of Epochs 148:weights = 2.330,loss=1.33749700\n",
            "Number of Epochs 149:weights = 2.335,loss=1.33499694\n",
            "Number of Epochs 150:weights = 2.325,loss=1.33500624\n",
            "Number of Epochs 151:weights = 2.330,loss=1.33749700\n",
            "Number of Epochs 152:weights = 2.335,loss=1.33499694\n",
            "Number of Epochs 153:weights = 2.325,loss=1.33500648\n",
            "Number of Epochs 154:weights = 2.330,loss=1.33749676\n",
            "Number of Epochs 155:weights = 2.335,loss=1.33499670\n",
            "Number of Epochs 156:weights = 2.325,loss=1.33500671\n",
            "Number of Epochs 157:weights = 2.330,loss=1.33749652\n",
            "Number of Epochs 158:weights = 2.335,loss=1.33499646\n",
            "Number of Epochs 159:weights = 2.325,loss=1.33500695\n",
            "Number of Epochs 160:weights = 2.330,loss=1.33749652\n",
            "Number of Epochs 161:weights = 2.335,loss=1.33499646\n",
            "Number of Epochs 162:weights = 2.325,loss=1.33500719\n",
            "Number of Epochs 163:weights = 2.330,loss=1.33749652\n",
            "Number of Epochs 164:weights = 2.335,loss=1.33499646\n",
            "Number of Epochs 165:weights = 2.325,loss=1.33500743\n",
            "Number of Epochs 166:weights = 2.330,loss=1.33749628\n",
            "Number of Epochs 167:weights = 2.335,loss=1.33499622\n",
            "Number of Epochs 168:weights = 2.325,loss=1.33500767\n",
            "Number of Epochs 169:weights = 2.330,loss=1.33749604\n",
            "Number of Epochs 170:weights = 2.335,loss=1.33499599\n",
            "Number of Epochs 171:weights = 2.325,loss=1.33500791\n",
            "Number of Epochs 172:weights = 2.330,loss=1.33749604\n",
            "Number of Epochs 173:weights = 2.335,loss=1.33499599\n",
            "Number of Epochs 174:weights = 2.325,loss=1.33500814\n",
            "Number of Epochs 175:weights = 2.330,loss=1.33749604\n",
            "Number of Epochs 176:weights = 2.335,loss=1.33499599\n",
            "Number of Epochs 177:weights = 2.325,loss=1.33500838\n",
            "Number of Epochs 178:weights = 2.330,loss=1.33749580\n",
            "Number of Epochs 179:weights = 2.335,loss=1.33499575\n",
            "Number of Epochs 180:weights = 2.325,loss=1.33500862\n",
            "Number of Epochs 181:weights = 2.330,loss=1.33749557\n",
            "Number of Epochs 182:weights = 2.335,loss=1.33499551\n",
            "Number of Epochs 183:weights = 2.325,loss=1.33500886\n",
            "Number of Epochs 184:weights = 2.330,loss=1.33749557\n",
            "Number of Epochs 185:weights = 2.335,loss=1.33499551\n",
            "Number of Epochs 186:weights = 2.325,loss=1.33500910\n",
            "Number of Epochs 187:weights = 2.330,loss=1.33749557\n",
            "Number of Epochs 188:weights = 2.335,loss=1.33499551\n",
            "Number of Epochs 189:weights = 2.325,loss=1.33500934\n",
            "Number of Epochs 190:weights = 2.330,loss=1.33749533\n",
            "Number of Epochs 191:weights = 2.335,loss=1.33499527\n",
            "Number of Epochs 192:weights = 2.325,loss=1.33500957\n",
            "Number of Epochs 193:weights = 2.330,loss=1.33749509\n",
            "Number of Epochs 194:weights = 2.335,loss=1.33499503\n",
            "Number of Epochs 195:weights = 2.325,loss=1.33500981\n",
            "Number of Epochs 196:weights = 2.330,loss=1.33749509\n",
            "Number of Epochs 197:weights = 2.335,loss=1.33499503\n",
            "Number of Epochs 198:weights = 2.325,loss=1.33501005\n",
            "Number of Epochs 199:weights = 2.330,loss=1.33749509\n",
            "Number of Epochs 200:weights = 2.335,loss=1.33499503\n",
            "Number of Epochs 201:weights = 2.325,loss=1.33501029\n",
            "Number of Epochs 202:weights = 2.330,loss=1.33749485\n",
            "Number of Epochs 203:weights = 2.335,loss=1.33499479\n",
            "Number of Epochs 204:weights = 2.325,loss=1.33501053\n",
            "Number of Epochs 205:weights = 2.330,loss=1.33749461\n",
            "Number of Epochs 206:weights = 2.335,loss=1.33499455\n",
            "Number of Epochs 207:weights = 2.325,loss=1.33501077\n",
            "Number of Epochs 208:weights = 2.330,loss=1.33749461\n",
            "Number of Epochs 209:weights = 2.335,loss=1.33499455\n",
            "Number of Epochs 210:weights = 2.325,loss=1.33501101\n",
            "Number of Epochs 211:weights = 2.330,loss=1.33749461\n",
            "Number of Epochs 212:weights = 2.335,loss=1.33499455\n",
            "Number of Epochs 213:weights = 2.325,loss=1.33501124\n",
            "Number of Epochs 214:weights = 2.330,loss=1.33749437\n",
            "Number of Epochs 215:weights = 2.335,loss=1.33499432\n",
            "Number of Epochs 216:weights = 2.325,loss=1.33501148\n",
            "Number of Epochs 217:weights = 2.330,loss=1.33749413\n",
            "Number of Epochs 218:weights = 2.335,loss=1.33499408\n",
            "Number of Epochs 219:weights = 2.325,loss=1.33501172\n",
            "Number of Epochs 220:weights = 2.330,loss=1.33749413\n",
            "Number of Epochs 221:weights = 2.335,loss=1.33499408\n",
            "Number of Epochs 222:weights = 2.325,loss=1.33501196\n",
            "Number of Epochs 223:weights = 2.330,loss=1.33749413\n",
            "Number of Epochs 224:weights = 2.335,loss=1.33499408\n",
            "Number of Epochs 225:weights = 2.325,loss=1.33501220\n",
            "Number of Epochs 226:weights = 2.330,loss=1.33749390\n",
            "Number of Epochs 227:weights = 2.335,loss=1.33499384\n",
            "Number of Epochs 228:weights = 2.325,loss=1.33501244\n",
            "Number of Epochs 229:weights = 2.330,loss=1.33749366\n",
            "Number of Epochs 230:weights = 2.335,loss=1.33499360\n",
            "Number of Epochs 231:weights = 2.325,loss=1.33501267\n",
            "Number of Epochs 232:weights = 2.330,loss=1.33749366\n",
            "Number of Epochs 233:weights = 2.335,loss=1.33499360\n",
            "Number of Epochs 234:weights = 2.325,loss=1.33501291\n",
            "Number of Epochs 235:weights = 2.330,loss=1.33749366\n",
            "Number of Epochs 236:weights = 2.335,loss=1.33499360\n",
            "Number of Epochs 237:weights = 2.325,loss=1.33501315\n",
            "Number of Epochs 238:weights = 2.330,loss=1.33749342\n",
            "Number of Epochs 239:weights = 2.335,loss=1.33499336\n",
            "Number of Epochs 240:weights = 2.325,loss=1.33501339\n",
            "Number of Epochs 241:weights = 2.330,loss=1.33749318\n",
            "Number of Epochs 242:weights = 2.335,loss=1.33499312\n",
            "Number of Epochs 243:weights = 2.325,loss=1.33501363\n",
            "Number of Epochs 244:weights = 2.330,loss=1.33749318\n",
            "Number of Epochs 245:weights = 2.335,loss=1.33499312\n",
            "Number of Epochs 246:weights = 2.325,loss=1.33501387\n",
            "Number of Epochs 247:weights = 2.330,loss=1.33749318\n",
            "Number of Epochs 248:weights = 2.335,loss=1.33499312\n",
            "Number of Epochs 249:weights = 2.325,loss=1.33501410\n",
            "Number of Epochs 250:weights = 2.330,loss=1.33749294\n",
            "Number of Epochs 251:weights = 2.335,loss=1.33499289\n",
            "Number of Epochs 252:weights = 2.325,loss=1.33501434\n",
            "Number of Epochs 253:weights = 2.330,loss=1.33749270\n",
            "Number of Epochs 254:weights = 2.335,loss=1.33499265\n",
            "Number of Epochs 255:weights = 2.325,loss=1.33501458\n",
            "Number of Epochs 256:weights = 2.330,loss=1.33749270\n",
            "Number of Epochs 257:weights = 2.335,loss=1.33499265\n",
            "Number of Epochs 258:weights = 2.325,loss=1.33501482\n",
            "Number of Epochs 259:weights = 2.330,loss=1.33749270\n",
            "Number of Epochs 260:weights = 2.335,loss=1.33499265\n",
            "Number of Epochs 261:weights = 2.325,loss=1.33501506\n",
            "Number of Epochs 262:weights = 2.330,loss=1.33749247\n",
            "Number of Epochs 263:weights = 2.335,loss=1.33499241\n",
            "Number of Epochs 264:weights = 2.325,loss=1.33501530\n",
            "Number of Epochs 265:weights = 2.330,loss=1.33749223\n",
            "Number of Epochs 266:weights = 2.335,loss=1.33499217\n",
            "Number of Epochs 267:weights = 2.325,loss=1.33501554\n",
            "Number of Epochs 268:weights = 2.330,loss=1.33749223\n",
            "Number of Epochs 269:weights = 2.335,loss=1.33499217\n",
            "Number of Epochs 270:weights = 2.325,loss=1.33501577\n",
            "Number of Epochs 271:weights = 2.330,loss=1.33749223\n",
            "Number of Epochs 272:weights = 2.335,loss=1.33499217\n",
            "Number of Epochs 273:weights = 2.325,loss=1.33501601\n",
            "Number of Epochs 274:weights = 2.330,loss=1.33749199\n",
            "Number of Epochs 275:weights = 2.335,loss=1.33499193\n",
            "Number of Epochs 276:weights = 2.325,loss=1.33501625\n",
            "Number of Epochs 277:weights = 2.330,loss=1.33749175\n",
            "Number of Epochs 278:weights = 2.335,loss=1.33499169\n",
            "Number of Epochs 279:weights = 2.325,loss=1.33501649\n",
            "Number of Epochs 280:weights = 2.330,loss=1.33749175\n",
            "Number of Epochs 281:weights = 2.335,loss=1.33499169\n",
            "Number of Epochs 282:weights = 2.325,loss=1.33501673\n",
            "Number of Epochs 283:weights = 2.330,loss=1.33749175\n",
            "Number of Epochs 284:weights = 2.335,loss=1.33499169\n",
            "Number of Epochs 285:weights = 2.325,loss=1.33501697\n",
            "Number of Epochs 286:weights = 2.330,loss=1.33749151\n",
            "Number of Epochs 287:weights = 2.335,loss=1.33499146\n",
            "Number of Epochs 288:weights = 2.325,loss=1.33501720\n",
            "Number of Epochs 289:weights = 2.330,loss=1.33749127\n",
            "Number of Epochs 290:weights = 2.335,loss=1.33499122\n",
            "Number of Epochs 291:weights = 2.325,loss=1.33501744\n",
            "Number of Epochs 292:weights = 2.330,loss=1.33749127\n",
            "Number of Epochs 293:weights = 2.335,loss=1.33499122\n",
            "Number of Epochs 294:weights = 2.325,loss=1.33501768\n",
            "Number of Epochs 295:weights = 2.330,loss=1.33749127\n",
            "Number of Epochs 296:weights = 2.335,loss=1.33499122\n",
            "Number of Epochs 297:weights = 2.325,loss=1.33501792\n",
            "Number of Epochs 298:weights = 2.330,loss=1.33749104\n",
            "Number of Epochs 299:weights = 2.335,loss=1.33499098\n",
            "Number of Epochs 300:weights = 2.325,loss=1.33501816\n",
            "Number of Epochs 301:weights = 2.330,loss=1.33749080\n",
            "Number of Epochs 302:weights = 2.335,loss=1.33499074\n",
            "Number of Epochs 303:weights = 2.325,loss=1.33501840\n",
            "Number of Epochs 304:weights = 2.330,loss=1.33749080\n",
            "Number of Epochs 305:weights = 2.335,loss=1.33499074\n",
            "Number of Epochs 306:weights = 2.325,loss=1.33501863\n",
            "Number of Epochs 307:weights = 2.330,loss=1.33749080\n",
            "Number of Epochs 308:weights = 2.335,loss=1.33499074\n",
            "Number of Epochs 309:weights = 2.325,loss=1.33501887\n",
            "Number of Epochs 310:weights = 2.330,loss=1.33749056\n",
            "Number of Epochs 311:weights = 2.335,loss=1.33499050\n",
            "Number of Epochs 312:weights = 2.325,loss=1.33501911\n",
            "Number of Epochs 313:weights = 2.330,loss=1.33749032\n",
            "Number of Epochs 314:weights = 2.335,loss=1.33499026\n",
            "Number of Epochs 315:weights = 2.325,loss=1.33501935\n",
            "Number of Epochs 316:weights = 2.330,loss=1.33749032\n",
            "Number of Epochs 317:weights = 2.335,loss=1.33499026\n",
            "Number of Epochs 318:weights = 2.325,loss=1.33501959\n",
            "Number of Epochs 319:weights = 2.330,loss=1.33749032\n",
            "Number of Epochs 320:weights = 2.335,loss=1.33499026\n",
            "Number of Epochs 321:weights = 2.325,loss=1.33501983\n",
            "Number of Epochs 322:weights = 2.330,loss=1.33749008\n",
            "Number of Epochs 323:weights = 2.335,loss=1.33499002\n",
            "Number of Epochs 324:weights = 2.325,loss=1.33502007\n",
            "Number of Epochs 325:weights = 2.330,loss=1.33748984\n",
            "Number of Epochs 326:weights = 2.335,loss=1.33498979\n",
            "Number of Epochs 327:weights = 2.325,loss=1.33502030\n",
            "Number of Epochs 328:weights = 2.330,loss=1.33748984\n",
            "Number of Epochs 329:weights = 2.335,loss=1.33498979\n",
            "Number of Epochs 330:weights = 2.325,loss=1.33502054\n",
            "Number of Epochs 331:weights = 2.330,loss=1.33748984\n",
            "Number of Epochs 332:weights = 2.335,loss=1.33498979\n",
            "Number of Epochs 333:weights = 2.325,loss=1.33502078\n",
            "Number of Epochs 334:weights = 2.330,loss=1.33748960\n",
            "Number of Epochs 335:weights = 2.335,loss=1.33498955\n",
            "Number of Epochs 336:weights = 2.325,loss=1.33502102\n",
            "Number of Epochs 337:weights = 2.330,loss=1.33748937\n",
            "Number of Epochs 338:weights = 2.335,loss=1.33498931\n",
            "Number of Epochs 339:weights = 2.325,loss=1.33502126\n",
            "Number of Epochs 340:weights = 2.330,loss=1.33748937\n",
            "Number of Epochs 341:weights = 2.335,loss=1.33498931\n",
            "Number of Epochs 342:weights = 2.325,loss=1.33502150\n",
            "Number of Epochs 343:weights = 2.330,loss=1.33748937\n",
            "Number of Epochs 344:weights = 2.335,loss=1.33498931\n",
            "Number of Epochs 345:weights = 2.325,loss=1.33502173\n",
            "Number of Epochs 346:weights = 2.330,loss=1.33748913\n",
            "Number of Epochs 347:weights = 2.335,loss=1.33498907\n",
            "Number of Epochs 348:weights = 2.325,loss=1.33502197\n",
            "Number of Epochs 349:weights = 2.330,loss=1.33748889\n",
            "Number of Epochs 350:weights = 2.335,loss=1.33498883\n",
            "Number of Epochs 351:weights = 2.325,loss=1.33502221\n",
            "Number of Epochs 352:weights = 2.330,loss=1.33748889\n",
            "Number of Epochs 353:weights = 2.335,loss=1.33498883\n",
            "Number of Epochs 354:weights = 2.325,loss=1.33502245\n",
            "Number of Epochs 355:weights = 2.330,loss=1.33748889\n",
            "Number of Epochs 356:weights = 2.335,loss=1.33498883\n",
            "Number of Epochs 357:weights = 2.325,loss=1.33502269\n",
            "Number of Epochs 358:weights = 2.330,loss=1.33748865\n",
            "Number of Epochs 359:weights = 2.335,loss=1.33498859\n",
            "Number of Epochs 360:weights = 2.325,loss=1.33502293\n",
            "Number of Epochs 361:weights = 2.330,loss=1.33748841\n",
            "Number of Epochs 362:weights = 2.335,loss=1.33498836\n",
            "Number of Epochs 363:weights = 2.325,loss=1.33502316\n",
            "Number of Epochs 364:weights = 2.330,loss=1.33748841\n",
            "Number of Epochs 365:weights = 2.335,loss=1.33498836\n",
            "Number of Epochs 366:weights = 2.325,loss=1.33502340\n",
            "Number of Epochs 367:weights = 2.330,loss=1.33748841\n",
            "Number of Epochs 368:weights = 2.335,loss=1.33498836\n",
            "Number of Epochs 369:weights = 2.325,loss=1.33502364\n",
            "Number of Epochs 370:weights = 2.330,loss=1.33748817\n",
            "Number of Epochs 371:weights = 2.335,loss=1.33498812\n",
            "Number of Epochs 372:weights = 2.325,loss=1.33502388\n",
            "Number of Epochs 373:weights = 2.330,loss=1.33748794\n",
            "Number of Epochs 374:weights = 2.335,loss=1.33498788\n",
            "Number of Epochs 375:weights = 2.325,loss=1.33502412\n",
            "Number of Epochs 376:weights = 2.330,loss=1.33748794\n",
            "Number of Epochs 377:weights = 2.335,loss=1.33498788\n",
            "Number of Epochs 378:weights = 2.325,loss=1.33502436\n",
            "Number of Epochs 379:weights = 2.330,loss=1.33748794\n",
            "Number of Epochs 380:weights = 2.335,loss=1.33498788\n",
            "Number of Epochs 381:weights = 2.325,loss=1.33502460\n",
            "Number of Epochs 382:weights = 2.330,loss=1.33748770\n",
            "Number of Epochs 383:weights = 2.335,loss=1.33498764\n",
            "Number of Epochs 384:weights = 2.325,loss=1.33502483\n",
            "Number of Epochs 385:weights = 2.330,loss=1.33748746\n",
            "Number of Epochs 386:weights = 2.335,loss=1.33498740\n",
            "Number of Epochs 387:weights = 2.325,loss=1.33502507\n",
            "Number of Epochs 388:weights = 2.330,loss=1.33748746\n",
            "Number of Epochs 389:weights = 2.335,loss=1.33498740\n",
            "Number of Epochs 390:weights = 2.325,loss=1.33502531\n",
            "Number of Epochs 391:weights = 2.330,loss=1.33748746\n",
            "Number of Epochs 392:weights = 2.335,loss=1.33498740\n",
            "Number of Epochs 393:weights = 2.325,loss=1.33502555\n",
            "Number of Epochs 394:weights = 2.330,loss=1.33748722\n",
            "Number of Epochs 395:weights = 2.335,loss=1.33498716\n",
            "Number of Epochs 396:weights = 2.325,loss=1.33502579\n",
            "Number of Epochs 397:weights = 2.330,loss=1.33748698\n",
            "Number of Epochs 398:weights = 2.335,loss=1.33498693\n",
            "Number of Epochs 399:weights = 2.325,loss=1.33502603\n",
            "Number of Epochs 400:weights = 2.330,loss=1.33748698\n",
            "Number of Epochs 401:weights = 2.335,loss=1.33498693\n",
            "Number of Epochs 402:weights = 2.325,loss=1.33502626\n",
            "Number of Epochs 403:weights = 2.330,loss=1.33748698\n",
            "Number of Epochs 404:weights = 2.335,loss=1.33498693\n",
            "Number of Epochs 405:weights = 2.325,loss=1.33502650\n",
            "Number of Epochs 406:weights = 2.330,loss=1.33748674\n",
            "Number of Epochs 407:weights = 2.335,loss=1.33498669\n",
            "Number of Epochs 408:weights = 2.325,loss=1.33502674\n",
            "Number of Epochs 409:weights = 2.330,loss=1.33748651\n",
            "Number of Epochs 410:weights = 2.335,loss=1.33498645\n",
            "Number of Epochs 411:weights = 2.325,loss=1.33502698\n",
            "Number of Epochs 412:weights = 2.330,loss=1.33748651\n",
            "Number of Epochs 413:weights = 2.335,loss=1.33498645\n",
            "Number of Epochs 414:weights = 2.325,loss=1.33502722\n",
            "Number of Epochs 415:weights = 2.330,loss=1.33748651\n",
            "Number of Epochs 416:weights = 2.335,loss=1.33498645\n",
            "Number of Epochs 417:weights = 2.325,loss=1.33502746\n",
            "Number of Epochs 418:weights = 2.330,loss=1.33748627\n",
            "Number of Epochs 419:weights = 2.335,loss=1.33498621\n",
            "Number of Epochs 420:weights = 2.325,loss=1.33502769\n",
            "Number of Epochs 421:weights = 2.330,loss=1.33748603\n",
            "Number of Epochs 422:weights = 2.335,loss=1.33498597\n",
            "Number of Epochs 423:weights = 2.325,loss=1.33502793\n",
            "Number of Epochs 424:weights = 2.330,loss=1.33748603\n",
            "Number of Epochs 425:weights = 2.335,loss=1.33498597\n",
            "Number of Epochs 426:weights = 2.325,loss=1.33502817\n",
            "Number of Epochs 427:weights = 2.330,loss=1.33748603\n",
            "Number of Epochs 428:weights = 2.335,loss=1.33498597\n",
            "Number of Epochs 429:weights = 2.325,loss=1.33502841\n",
            "Number of Epochs 430:weights = 2.330,loss=1.33748579\n",
            "Number of Epochs 431:weights = 2.335,loss=1.33498573\n",
            "Number of Epochs 432:weights = 2.325,loss=1.33502865\n",
            "Number of Epochs 433:weights = 2.330,loss=1.33748555\n",
            "Number of Epochs 434:weights = 2.335,loss=1.33498549\n",
            "Number of Epochs 435:weights = 2.325,loss=1.33502889\n",
            "Number of Epochs 436:weights = 2.330,loss=1.33748555\n",
            "Number of Epochs 437:weights = 2.335,loss=1.33498549\n",
            "Number of Epochs 438:weights = 2.325,loss=1.33502913\n",
            "Number of Epochs 439:weights = 2.330,loss=1.33748555\n",
            "Number of Epochs 440:weights = 2.335,loss=1.33498549\n",
            "Number of Epochs 441:weights = 2.325,loss=1.33502936\n",
            "Number of Epochs 442:weights = 2.330,loss=1.33748531\n",
            "Number of Epochs 443:weights = 2.335,loss=1.33498526\n",
            "Number of Epochs 444:weights = 2.325,loss=1.33502960\n",
            "Number of Epochs 445:weights = 2.330,loss=1.33748507\n",
            "Number of Epochs 446:weights = 2.335,loss=1.33498502\n",
            "Number of Epochs 447:weights = 2.325,loss=1.33502984\n",
            "Number of Epochs 448:weights = 2.330,loss=1.33748507\n",
            "Number of Epochs 449:weights = 2.335,loss=1.33498502\n",
            "Number of Epochs 450:weights = 2.325,loss=1.33503008\n",
            "Number of Epochs 451:weights = 2.330,loss=1.33748507\n",
            "Number of Epochs 452:weights = 2.335,loss=1.33498502\n",
            "Number of Epochs 453:weights = 2.325,loss=1.33503032\n",
            "Number of Epochs 454:weights = 2.330,loss=1.33748484\n",
            "Number of Epochs 455:weights = 2.335,loss=1.33498478\n",
            "Number of Epochs 456:weights = 2.325,loss=1.33503056\n",
            "Number of Epochs 457:weights = 2.330,loss=1.33748460\n",
            "Number of Epochs 458:weights = 2.335,loss=1.33498454\n",
            "Number of Epochs 459:weights = 2.325,loss=1.33503079\n",
            "Number of Epochs 460:weights = 2.330,loss=1.33748460\n",
            "Number of Epochs 461:weights = 2.335,loss=1.33498454\n",
            "Number of Epochs 462:weights = 2.325,loss=1.33503103\n",
            "Number of Epochs 463:weights = 2.330,loss=1.33748460\n",
            "Number of Epochs 464:weights = 2.335,loss=1.33498454\n",
            "Number of Epochs 465:weights = 2.325,loss=1.33503127\n",
            "Number of Epochs 466:weights = 2.330,loss=1.33748436\n",
            "Number of Epochs 467:weights = 2.335,loss=1.33498430\n",
            "Number of Epochs 468:weights = 2.325,loss=1.33503151\n",
            "Number of Epochs 469:weights = 2.330,loss=1.33748412\n",
            "Number of Epochs 470:weights = 2.335,loss=1.33498406\n",
            "Number of Epochs 471:weights = 2.325,loss=1.33503175\n",
            "Number of Epochs 472:weights = 2.330,loss=1.33748412\n",
            "Number of Epochs 473:weights = 2.335,loss=1.33498406\n",
            "Number of Epochs 474:weights = 2.325,loss=1.33503199\n",
            "Number of Epochs 475:weights = 2.330,loss=1.33748412\n",
            "Number of Epochs 476:weights = 2.335,loss=1.33498406\n",
            "Number of Epochs 477:weights = 2.325,loss=1.33503222\n",
            "Number of Epochs 478:weights = 2.330,loss=1.33748388\n",
            "Number of Epochs 479:weights = 2.335,loss=1.33498383\n",
            "Number of Epochs 480:weights = 2.325,loss=1.33503246\n",
            "Number of Epochs 481:weights = 2.330,loss=1.33748364\n",
            "Number of Epochs 482:weights = 2.335,loss=1.33498359\n",
            "Number of Epochs 483:weights = 2.325,loss=1.33503270\n",
            "Number of Epochs 484:weights = 2.330,loss=1.33748364\n",
            "Number of Epochs 485:weights = 2.335,loss=1.33498359\n",
            "Number of Epochs 486:weights = 2.325,loss=1.33503294\n",
            "Number of Epochs 487:weights = 2.330,loss=1.33748364\n",
            "Number of Epochs 488:weights = 2.335,loss=1.33498359\n",
            "Number of Epochs 489:weights = 2.325,loss=1.33503318\n",
            "Number of Epochs 490:weights = 2.330,loss=1.33748341\n",
            "Number of Epochs 491:weights = 2.335,loss=1.33498335\n",
            "Number of Epochs 492:weights = 2.325,loss=1.33503342\n",
            "Number of Epochs 493:weights = 2.330,loss=1.33748317\n",
            "Number of Epochs 494:weights = 2.335,loss=1.33498311\n",
            "Number of Epochs 495:weights = 2.325,loss=1.33503366\n",
            "Number of Epochs 496:weights = 2.330,loss=1.33748317\n",
            "Number of Epochs 497:weights = 2.335,loss=1.33498311\n",
            "Number of Epochs 498:weights = 2.325,loss=1.33503389\n",
            "Number of Epochs 499:weights = 2.330,loss=1.33748317\n",
            "Number of Epochs 500:weights = 2.335,loss=1.33498311\n",
            "Number of Epochs 501:weights = 2.325,loss=1.33503413\n",
            "Number of Epochs 502:weights = 2.330,loss=1.33748293\n",
            "Number of Epochs 503:weights = 2.335,loss=1.33498287\n",
            "Number of Epochs 504:weights = 2.325,loss=1.33503437\n",
            "Number of Epochs 505:weights = 2.330,loss=1.33748269\n",
            "Number of Epochs 506:weights = 2.335,loss=1.33498263\n",
            "Number of Epochs 507:weights = 2.325,loss=1.33503461\n",
            "Number of Epochs 508:weights = 2.330,loss=1.33748269\n",
            "Number of Epochs 509:weights = 2.335,loss=1.33498263\n",
            "Number of Epochs 510:weights = 2.325,loss=1.33503485\n",
            "Number of Epochs 511:weights = 2.330,loss=1.33748269\n",
            "Number of Epochs 512:weights = 2.335,loss=1.33498263\n",
            "Number of Epochs 513:weights = 2.325,loss=1.33503509\n",
            "Number of Epochs 514:weights = 2.330,loss=1.33748245\n",
            "Number of Epochs 515:weights = 2.335,loss=1.33498240\n",
            "Number of Epochs 516:weights = 2.325,loss=1.33503532\n",
            "Number of Epochs 517:weights = 2.330,loss=1.33748221\n",
            "Number of Epochs 518:weights = 2.335,loss=1.33498216\n",
            "Number of Epochs 519:weights = 2.325,loss=1.33503556\n",
            "Number of Epochs 520:weights = 2.330,loss=1.33748221\n",
            "Number of Epochs 521:weights = 2.335,loss=1.33498216\n",
            "Number of Epochs 522:weights = 2.325,loss=1.33503580\n",
            "Number of Epochs 523:weights = 2.330,loss=1.33748221\n",
            "Number of Epochs 524:weights = 2.335,loss=1.33498216\n",
            "Number of Epochs 525:weights = 2.325,loss=1.33503604\n",
            "Number of Epochs 526:weights = 2.330,loss=1.33748198\n",
            "Number of Epochs 527:weights = 2.335,loss=1.33498192\n",
            "Number of Epochs 528:weights = 2.325,loss=1.33503628\n",
            "Number of Epochs 529:weights = 2.330,loss=1.33748174\n",
            "Number of Epochs 530:weights = 2.335,loss=1.33498168\n",
            "Number of Epochs 531:weights = 2.325,loss=1.33503652\n",
            "Number of Epochs 532:weights = 2.330,loss=1.33748174\n",
            "Number of Epochs 533:weights = 2.335,loss=1.33498168\n",
            "Number of Epochs 534:weights = 2.325,loss=1.33503675\n",
            "Number of Epochs 535:weights = 2.330,loss=1.33748174\n",
            "Number of Epochs 536:weights = 2.335,loss=1.33498168\n",
            "Number of Epochs 537:weights = 2.325,loss=1.33503699\n",
            "Number of Epochs 538:weights = 2.330,loss=1.33748150\n",
            "Number of Epochs 539:weights = 2.335,loss=1.33498144\n",
            "Number of Epochs 540:weights = 2.325,loss=1.33503723\n",
            "Number of Epochs 541:weights = 2.330,loss=1.33748126\n",
            "Number of Epochs 542:weights = 2.335,loss=1.33498120\n",
            "Number of Epochs 543:weights = 2.325,loss=1.33503747\n",
            "Number of Epochs 544:weights = 2.330,loss=1.33748126\n",
            "Number of Epochs 545:weights = 2.335,loss=1.33498120\n",
            "Number of Epochs 546:weights = 2.325,loss=1.33503771\n",
            "Number of Epochs 547:weights = 2.330,loss=1.33748126\n",
            "Number of Epochs 548:weights = 2.335,loss=1.33498120\n",
            "Number of Epochs 549:weights = 2.325,loss=1.33503795\n",
            "Number of Epochs 550:weights = 2.330,loss=1.33748102\n",
            "Number of Epochs 551:weights = 2.335,loss=1.33498096\n",
            "Number of Epochs 552:weights = 2.325,loss=1.33503819\n",
            "Number of Epochs 553:weights = 2.330,loss=1.33748078\n",
            "Number of Epochs 554:weights = 2.335,loss=1.33498073\n",
            "Number of Epochs 555:weights = 2.325,loss=1.33503842\n",
            "Number of Epochs 556:weights = 2.330,loss=1.33748078\n",
            "Number of Epochs 557:weights = 2.335,loss=1.33498073\n",
            "Number of Epochs 558:weights = 2.325,loss=1.33503866\n",
            "Number of Epochs 559:weights = 2.330,loss=1.33748078\n",
            "Number of Epochs 560:weights = 2.335,loss=1.33498073\n",
            "Number of Epochs 561:weights = 2.325,loss=1.33503890\n",
            "Number of Epochs 562:weights = 2.330,loss=1.33748055\n",
            "Number of Epochs 563:weights = 2.335,loss=1.33498049\n",
            "Number of Epochs 564:weights = 2.325,loss=1.33503914\n",
            "Number of Epochs 565:weights = 2.330,loss=1.33748031\n",
            "Number of Epochs 566:weights = 2.335,loss=1.33498025\n",
            "Number of Epochs 567:weights = 2.325,loss=1.33503938\n",
            "Number of Epochs 568:weights = 2.330,loss=1.33748031\n",
            "Number of Epochs 569:weights = 2.335,loss=1.33498025\n",
            "Number of Epochs 570:weights = 2.325,loss=1.33503962\n",
            "Number of Epochs 571:weights = 2.330,loss=1.33748031\n",
            "Number of Epochs 572:weights = 2.335,loss=1.33498025\n",
            "Number of Epochs 573:weights = 2.325,loss=1.33503985\n",
            "Number of Epochs 574:weights = 2.330,loss=1.33748007\n",
            "Number of Epochs 575:weights = 2.335,loss=1.33498001\n",
            "Number of Epochs 576:weights = 2.325,loss=1.33504009\n",
            "Number of Epochs 577:weights = 2.330,loss=1.33747983\n",
            "Number of Epochs 578:weights = 2.335,loss=1.33497977\n",
            "Number of Epochs 579:weights = 2.325,loss=1.33504033\n",
            "Number of Epochs 580:weights = 2.330,loss=1.33747983\n",
            "Number of Epochs 581:weights = 2.335,loss=1.33497977\n",
            "Number of Epochs 582:weights = 2.325,loss=1.33504057\n",
            "Number of Epochs 583:weights = 2.330,loss=1.33747983\n",
            "Number of Epochs 584:weights = 2.335,loss=1.33497977\n",
            "Number of Epochs 585:weights = 2.325,loss=1.33504081\n",
            "Number of Epochs 586:weights = 2.330,loss=1.33747959\n",
            "Number of Epochs 587:weights = 2.335,loss=1.33497953\n",
            "Number of Epochs 588:weights = 2.325,loss=1.33504105\n",
            "Number of Epochs 589:weights = 2.330,loss=1.33747935\n",
            "Number of Epochs 590:weights = 2.335,loss=1.33497930\n",
            "Number of Epochs 591:weights = 2.325,loss=1.33504128\n",
            "Number of Epochs 592:weights = 2.330,loss=1.33747935\n",
            "Number of Epochs 593:weights = 2.335,loss=1.33497930\n",
            "Number of Epochs 594:weights = 2.325,loss=1.33504152\n",
            "Number of Epochs 595:weights = 2.330,loss=1.33747935\n",
            "Number of Epochs 596:weights = 2.335,loss=1.33497930\n",
            "Number of Epochs 597:weights = 2.325,loss=1.33504176\n",
            "Number of Epochs 598:weights = 2.330,loss=1.33747911\n",
            "Number of Epochs 599:weights = 2.335,loss=1.33497906\n",
            "Number of Epochs 600:weights = 2.325,loss=1.33504200\n",
            "Number of Epochs 601:weights = 2.330,loss=1.33747888\n",
            "Number of Epochs 602:weights = 2.335,loss=1.33497882\n",
            "Number of Epochs 603:weights = 2.325,loss=1.33504224\n",
            "Number of Epochs 604:weights = 2.330,loss=1.33747888\n",
            "Number of Epochs 605:weights = 2.335,loss=1.33497882\n",
            "Number of Epochs 606:weights = 2.325,loss=1.33504248\n",
            "Number of Epochs 607:weights = 2.330,loss=1.33747888\n",
            "Number of Epochs 608:weights = 2.335,loss=1.33497882\n",
            "Number of Epochs 609:weights = 2.325,loss=1.33504272\n",
            "Number of Epochs 610:weights = 2.330,loss=1.33747864\n",
            "Number of Epochs 611:weights = 2.335,loss=1.33497858\n",
            "Number of Epochs 612:weights = 2.325,loss=1.33504295\n",
            "Number of Epochs 613:weights = 2.330,loss=1.33747840\n",
            "Number of Epochs 614:weights = 2.335,loss=1.33497834\n",
            "Number of Epochs 615:weights = 2.325,loss=1.33504319\n",
            "Number of Epochs 616:weights = 2.330,loss=1.33747840\n",
            "Number of Epochs 617:weights = 2.335,loss=1.33497834\n",
            "Number of Epochs 618:weights = 2.325,loss=1.33504343\n",
            "Number of Epochs 619:weights = 2.330,loss=1.33747840\n",
            "Number of Epochs 620:weights = 2.335,loss=1.33497834\n",
            "Number of Epochs 621:weights = 2.325,loss=1.33504367\n",
            "Number of Epochs 622:weights = 2.330,loss=1.33747816\n",
            "Number of Epochs 623:weights = 2.335,loss=1.33497810\n",
            "Number of Epochs 624:weights = 2.325,loss=1.33504391\n",
            "Number of Epochs 625:weights = 2.330,loss=1.33747792\n",
            "Number of Epochs 626:weights = 2.335,loss=1.33497787\n",
            "Number of Epochs 627:weights = 2.325,loss=1.33504415\n",
            "Number of Epochs 628:weights = 2.330,loss=1.33747792\n",
            "Number of Epochs 629:weights = 2.335,loss=1.33497787\n",
            "Number of Epochs 630:weights = 2.325,loss=1.33504438\n",
            "Number of Epochs 631:weights = 2.330,loss=1.33747792\n",
            "Number of Epochs 632:weights = 2.335,loss=1.33497787\n",
            "Number of Epochs 633:weights = 2.325,loss=1.33504462\n",
            "Number of Epochs 634:weights = 2.330,loss=1.33747768\n",
            "Number of Epochs 635:weights = 2.335,loss=1.33497763\n",
            "Number of Epochs 636:weights = 2.325,loss=1.33504486\n",
            "Number of Epochs 637:weights = 2.330,loss=1.33747745\n",
            "Number of Epochs 638:weights = 2.335,loss=1.33497739\n",
            "Number of Epochs 639:weights = 2.325,loss=1.33504510\n",
            "Number of Epochs 640:weights = 2.330,loss=1.33747745\n",
            "Number of Epochs 641:weights = 2.335,loss=1.33497739\n",
            "Number of Epochs 642:weights = 2.325,loss=1.33504534\n",
            "Number of Epochs 643:weights = 2.330,loss=1.33747745\n",
            "Number of Epochs 644:weights = 2.335,loss=1.33497739\n",
            "Number of Epochs 645:weights = 2.325,loss=1.33504558\n",
            "Number of Epochs 646:weights = 2.330,loss=1.33747721\n",
            "Number of Epochs 647:weights = 2.335,loss=1.33497715\n",
            "Number of Epochs 648:weights = 2.325,loss=1.33504581\n",
            "Number of Epochs 649:weights = 2.330,loss=1.33747697\n",
            "Number of Epochs 650:weights = 2.335,loss=1.33497691\n",
            "Number of Epochs 651:weights = 2.325,loss=1.33504605\n",
            "Number of Epochs 652:weights = 2.330,loss=1.33747697\n",
            "Number of Epochs 653:weights = 2.335,loss=1.33497691\n",
            "Number of Epochs 654:weights = 2.325,loss=1.33504629\n",
            "Number of Epochs 655:weights = 2.330,loss=1.33747697\n",
            "Number of Epochs 656:weights = 2.335,loss=1.33497691\n",
            "Number of Epochs 657:weights = 2.325,loss=1.33504653\n",
            "Number of Epochs 658:weights = 2.330,loss=1.33747673\n",
            "Number of Epochs 659:weights = 2.335,loss=1.33497667\n",
            "Number of Epochs 660:weights = 2.325,loss=1.33504677\n",
            "Number of Epochs 661:weights = 2.330,loss=1.33747649\n",
            "Number of Epochs 662:weights = 2.335,loss=1.33497643\n",
            "Number of Epochs 663:weights = 2.325,loss=1.33504701\n",
            "Number of Epochs 664:weights = 2.330,loss=1.33747649\n",
            "Number of Epochs 665:weights = 2.335,loss=1.33497643\n",
            "Number of Epochs 666:weights = 2.325,loss=1.33504725\n",
            "Number of Epochs 667:weights = 2.330,loss=1.33747649\n",
            "Number of Epochs 668:weights = 2.335,loss=1.33497643\n",
            "Number of Epochs 669:weights = 2.325,loss=1.33504748\n",
            "Number of Epochs 670:weights = 2.330,loss=1.33747625\n",
            "Number of Epochs 671:weights = 2.335,loss=1.33497620\n",
            "Number of Epochs 672:weights = 2.325,loss=1.33504772\n",
            "Number of Epochs 673:weights = 2.330,loss=1.33747602\n",
            "Number of Epochs 674:weights = 2.335,loss=1.33497596\n",
            "Number of Epochs 675:weights = 2.325,loss=1.33504796\n",
            "Number of Epochs 676:weights = 2.330,loss=1.33747602\n",
            "Number of Epochs 677:weights = 2.335,loss=1.33497596\n",
            "Number of Epochs 678:weights = 2.325,loss=1.33504820\n",
            "Number of Epochs 679:weights = 2.330,loss=1.33747602\n",
            "Number of Epochs 680:weights = 2.335,loss=1.33497596\n",
            "Number of Epochs 681:weights = 2.325,loss=1.33504844\n",
            "Number of Epochs 682:weights = 2.330,loss=1.33747578\n",
            "Number of Epochs 683:weights = 2.335,loss=1.33497572\n",
            "Number of Epochs 684:weights = 2.325,loss=1.33504868\n",
            "Number of Epochs 685:weights = 2.330,loss=1.33747554\n",
            "Number of Epochs 686:weights = 2.335,loss=1.33497548\n",
            "Number of Epochs 687:weights = 2.325,loss=1.33504891\n",
            "Number of Epochs 688:weights = 2.330,loss=1.33747554\n",
            "Number of Epochs 689:weights = 2.335,loss=1.33497548\n",
            "Number of Epochs 690:weights = 2.325,loss=1.33504915\n",
            "Number of Epochs 691:weights = 2.330,loss=1.33747554\n",
            "Number of Epochs 692:weights = 2.335,loss=1.33497548\n",
            "Number of Epochs 693:weights = 2.325,loss=1.33504939\n",
            "Number of Epochs 694:weights = 2.330,loss=1.33747530\n",
            "Number of Epochs 695:weights = 2.335,loss=1.33497524\n",
            "Number of Epochs 696:weights = 2.325,loss=1.33504963\n",
            "Number of Epochs 697:weights = 2.330,loss=1.33747506\n",
            "Number of Epochs 698:weights = 2.335,loss=1.33497500\n",
            "Number of Epochs 699:weights = 2.325,loss=1.33504987\n",
            "Number of Epochs 700:weights = 2.330,loss=1.33747506\n",
            "Number of Epochs 701:weights = 2.335,loss=1.33497500\n",
            "Number of Epochs 702:weights = 2.325,loss=1.33505011\n",
            "Number of Epochs 703:weights = 2.330,loss=1.33747506\n",
            "Number of Epochs 704:weights = 2.335,loss=1.33497500\n",
            "Number of Epochs 705:weights = 2.325,loss=1.33505034\n",
            "Number of Epochs 706:weights = 2.330,loss=1.33747482\n",
            "Number of Epochs 707:weights = 2.335,loss=1.33497477\n",
            "Number of Epochs 708:weights = 2.325,loss=1.33505058\n",
            "Number of Epochs 709:weights = 2.330,loss=1.33747458\n",
            "Number of Epochs 710:weights = 2.335,loss=1.33497453\n",
            "Number of Epochs 711:weights = 2.325,loss=1.33505082\n",
            "Number of Epochs 712:weights = 2.330,loss=1.33747458\n",
            "Number of Epochs 713:weights = 2.335,loss=1.33497453\n",
            "Number of Epochs 714:weights = 2.325,loss=1.33505106\n",
            "Number of Epochs 715:weights = 2.330,loss=1.33747458\n",
            "Number of Epochs 716:weights = 2.335,loss=1.33497453\n",
            "Number of Epochs 717:weights = 2.325,loss=1.33505130\n",
            "Number of Epochs 718:weights = 2.330,loss=1.33747435\n",
            "Number of Epochs 719:weights = 2.335,loss=1.33497429\n",
            "Number of Epochs 720:weights = 2.325,loss=1.33505154\n",
            "Number of Epochs 721:weights = 2.330,loss=1.33747411\n",
            "Number of Epochs 722:weights = 2.335,loss=1.33497405\n",
            "Number of Epochs 723:weights = 2.325,loss=1.33505177\n",
            "Number of Epochs 724:weights = 2.330,loss=1.33747411\n",
            "Number of Epochs 725:weights = 2.335,loss=1.33497405\n",
            "Number of Epochs 726:weights = 2.325,loss=1.33505201\n",
            "Number of Epochs 727:weights = 2.330,loss=1.33747411\n",
            "Number of Epochs 728:weights = 2.335,loss=1.33497405\n",
            "Number of Epochs 729:weights = 2.325,loss=1.33505225\n",
            "Number of Epochs 730:weights = 2.330,loss=1.33747387\n",
            "Number of Epochs 731:weights = 2.335,loss=1.33497381\n",
            "Number of Epochs 732:weights = 2.325,loss=1.33505249\n",
            "Number of Epochs 733:weights = 2.330,loss=1.33747363\n",
            "Number of Epochs 734:weights = 2.335,loss=1.33497357\n",
            "Number of Epochs 735:weights = 2.325,loss=1.33505273\n",
            "Number of Epochs 736:weights = 2.330,loss=1.33747363\n",
            "Number of Epochs 737:weights = 2.335,loss=1.33497357\n",
            "Number of Epochs 738:weights = 2.325,loss=1.33505297\n",
            "Number of Epochs 739:weights = 2.330,loss=1.33747363\n",
            "Number of Epochs 740:weights = 2.335,loss=1.33497357\n",
            "Number of Epochs 741:weights = 2.325,loss=1.33505321\n",
            "Number of Epochs 742:weights = 2.330,loss=1.33747339\n",
            "Number of Epochs 743:weights = 2.335,loss=1.33497334\n",
            "Number of Epochs 744:weights = 2.325,loss=1.33505344\n",
            "Number of Epochs 745:weights = 2.330,loss=1.33747315\n",
            "Number of Epochs 746:weights = 2.335,loss=1.33497310\n",
            "Number of Epochs 747:weights = 2.325,loss=1.33505368\n",
            "Number of Epochs 748:weights = 2.330,loss=1.33747315\n",
            "Number of Epochs 749:weights = 2.335,loss=1.33497310\n",
            "Number of Epochs 750:weights = 2.325,loss=1.33505392\n",
            "Number of Epochs 751:weights = 2.330,loss=1.33747315\n",
            "Number of Epochs 752:weights = 2.335,loss=1.33497310\n",
            "Number of Epochs 753:weights = 2.325,loss=1.33505416\n",
            "Number of Epochs 754:weights = 2.330,loss=1.33747292\n",
            "Number of Epochs 755:weights = 2.335,loss=1.33497286\n",
            "Number of Epochs 756:weights = 2.325,loss=1.33505440\n",
            "Number of Epochs 757:weights = 2.330,loss=1.33747268\n",
            "Number of Epochs 758:weights = 2.335,loss=1.33497262\n",
            "Number of Epochs 759:weights = 2.325,loss=1.33505464\n",
            "Number of Epochs 760:weights = 2.330,loss=1.33747268\n",
            "Number of Epochs 761:weights = 2.335,loss=1.33497262\n",
            "Number of Epochs 762:weights = 2.325,loss=1.33505487\n",
            "Number of Epochs 763:weights = 2.330,loss=1.33747268\n",
            "Number of Epochs 764:weights = 2.335,loss=1.33497262\n",
            "Number of Epochs 765:weights = 2.325,loss=1.33505511\n",
            "Number of Epochs 766:weights = 2.330,loss=1.33747244\n",
            "Number of Epochs 767:weights = 2.335,loss=1.33497238\n",
            "Number of Epochs 768:weights = 2.325,loss=1.33505535\n",
            "Number of Epochs 769:weights = 2.330,loss=1.33747220\n",
            "Number of Epochs 770:weights = 2.335,loss=1.33497214\n",
            "Number of Epochs 771:weights = 2.325,loss=1.33505559\n",
            "Number of Epochs 772:weights = 2.330,loss=1.33747220\n",
            "Number of Epochs 773:weights = 2.335,loss=1.33497214\n",
            "Number of Epochs 774:weights = 2.325,loss=1.33505583\n",
            "Number of Epochs 775:weights = 2.330,loss=1.33747220\n",
            "Number of Epochs 776:weights = 2.335,loss=1.33497214\n",
            "Number of Epochs 777:weights = 2.325,loss=1.33505607\n",
            "Number of Epochs 778:weights = 2.330,loss=1.33747196\n",
            "Number of Epochs 779:weights = 2.335,loss=1.33497190\n",
            "Number of Epochs 780:weights = 2.325,loss=1.33505630\n",
            "Number of Epochs 781:weights = 2.330,loss=1.33747172\n",
            "Number of Epochs 782:weights = 2.335,loss=1.33497167\n",
            "Number of Epochs 783:weights = 2.325,loss=1.33505654\n",
            "Number of Epochs 784:weights = 2.330,loss=1.33747172\n",
            "Number of Epochs 785:weights = 2.335,loss=1.33497167\n",
            "Number of Epochs 786:weights = 2.325,loss=1.33505678\n",
            "Number of Epochs 787:weights = 2.330,loss=1.33747172\n",
            "Number of Epochs 788:weights = 2.335,loss=1.33497167\n",
            "Number of Epochs 789:weights = 2.325,loss=1.33505702\n",
            "Number of Epochs 790:weights = 2.330,loss=1.33747149\n",
            "Number of Epochs 791:weights = 2.335,loss=1.33497143\n",
            "Number of Epochs 792:weights = 2.325,loss=1.33505726\n",
            "Number of Epochs 793:weights = 2.330,loss=1.33747125\n",
            "Number of Epochs 794:weights = 2.335,loss=1.33497119\n",
            "Number of Epochs 795:weights = 2.325,loss=1.33505750\n",
            "Number of Epochs 796:weights = 2.330,loss=1.33747125\n",
            "Number of Epochs 797:weights = 2.335,loss=1.33497119\n",
            "Number of Epochs 798:weights = 2.325,loss=1.33505774\n",
            "Number of Epochs 799:weights = 2.330,loss=1.33747125\n",
            "Number of Epochs 800:weights = 2.335,loss=1.33497119\n",
            "Number of Epochs 801:weights = 2.325,loss=1.33505797\n",
            "Number of Epochs 802:weights = 2.330,loss=1.33747101\n",
            "Number of Epochs 803:weights = 2.335,loss=1.33497095\n",
            "Number of Epochs 804:weights = 2.325,loss=1.33505821\n",
            "Number of Epochs 805:weights = 2.330,loss=1.33747077\n",
            "Number of Epochs 806:weights = 2.335,loss=1.33497071\n",
            "Number of Epochs 807:weights = 2.325,loss=1.33505845\n",
            "Number of Epochs 808:weights = 2.330,loss=1.33747077\n",
            "Number of Epochs 809:weights = 2.335,loss=1.33497071\n",
            "Number of Epochs 810:weights = 2.325,loss=1.33505869\n",
            "Number of Epochs 811:weights = 2.330,loss=1.33747077\n",
            "Number of Epochs 812:weights = 2.335,loss=1.33497071\n",
            "Number of Epochs 813:weights = 2.325,loss=1.33505893\n",
            "Number of Epochs 814:weights = 2.330,loss=1.33747053\n",
            "Number of Epochs 815:weights = 2.335,loss=1.33497047\n",
            "Number of Epochs 816:weights = 2.325,loss=1.33505917\n",
            "Number of Epochs 817:weights = 2.330,loss=1.33747029\n",
            "Number of Epochs 818:weights = 2.335,loss=1.33497024\n",
            "Number of Epochs 819:weights = 2.325,loss=1.33505940\n",
            "Number of Epochs 820:weights = 2.330,loss=1.33747029\n",
            "Number of Epochs 821:weights = 2.335,loss=1.33497024\n",
            "Number of Epochs 822:weights = 2.325,loss=1.33505964\n",
            "Number of Epochs 823:weights = 2.330,loss=1.33747029\n",
            "Number of Epochs 824:weights = 2.335,loss=1.33497024\n",
            "Number of Epochs 825:weights = 2.325,loss=1.33505988\n",
            "Number of Epochs 826:weights = 2.330,loss=1.33747005\n",
            "Number of Epochs 827:weights = 2.335,loss=1.33497000\n",
            "Number of Epochs 828:weights = 2.325,loss=1.33506012\n",
            "Number of Epochs 829:weights = 2.330,loss=1.33746982\n",
            "Number of Epochs 830:weights = 2.335,loss=1.33496976\n",
            "Number of Epochs 831:weights = 2.325,loss=1.33506036\n",
            "Number of Epochs 832:weights = 2.330,loss=1.33746982\n",
            "Number of Epochs 833:weights = 2.335,loss=1.33496976\n",
            "Number of Epochs 834:weights = 2.325,loss=1.33506060\n",
            "Number of Epochs 835:weights = 2.330,loss=1.33746982\n",
            "Number of Epochs 836:weights = 2.335,loss=1.33496976\n",
            "Number of Epochs 837:weights = 2.325,loss=1.33506083\n",
            "Number of Epochs 838:weights = 2.330,loss=1.33746958\n",
            "Number of Epochs 839:weights = 2.335,loss=1.33496952\n",
            "Number of Epochs 840:weights = 2.325,loss=1.33506107\n",
            "Number of Epochs 841:weights = 2.330,loss=1.33746934\n",
            "Number of Epochs 842:weights = 2.335,loss=1.33496928\n",
            "Number of Epochs 843:weights = 2.325,loss=1.33506131\n",
            "Number of Epochs 844:weights = 2.330,loss=1.33746934\n",
            "Number of Epochs 845:weights = 2.335,loss=1.33496928\n",
            "Number of Epochs 846:weights = 2.325,loss=1.33506155\n",
            "Number of Epochs 847:weights = 2.330,loss=1.33746934\n",
            "Number of Epochs 848:weights = 2.335,loss=1.33496928\n",
            "Number of Epochs 849:weights = 2.325,loss=1.33506179\n",
            "Number of Epochs 850:weights = 2.330,loss=1.33746910\n",
            "Number of Epochs 851:weights = 2.335,loss=1.33496904\n",
            "Number of Epochs 852:weights = 2.325,loss=1.33506203\n",
            "Number of Epochs 853:weights = 2.330,loss=1.33746886\n",
            "Number of Epochs 854:weights = 2.335,loss=1.33496881\n",
            "Number of Epochs 855:weights = 2.325,loss=1.33506227\n",
            "Number of Epochs 856:weights = 2.330,loss=1.33746886\n",
            "Number of Epochs 857:weights = 2.335,loss=1.33496881\n",
            "Number of Epochs 858:weights = 2.325,loss=1.33506250\n",
            "Number of Epochs 859:weights = 2.330,loss=1.33746886\n",
            "Number of Epochs 860:weights = 2.335,loss=1.33496881\n",
            "Number of Epochs 861:weights = 2.325,loss=1.33506274\n",
            "Number of Epochs 862:weights = 2.330,loss=1.33746862\n",
            "Number of Epochs 863:weights = 2.335,loss=1.33496857\n",
            "Number of Epochs 864:weights = 2.325,loss=1.33506298\n",
            "Number of Epochs 865:weights = 2.330,loss=1.33746839\n",
            "Number of Epochs 866:weights = 2.335,loss=1.33496833\n",
            "Number of Epochs 867:weights = 2.325,loss=1.33506322\n",
            "Number of Epochs 868:weights = 2.330,loss=1.33746839\n",
            "Number of Epochs 869:weights = 2.335,loss=1.33496833\n",
            "Number of Epochs 870:weights = 2.325,loss=1.33506346\n",
            "Number of Epochs 871:weights = 2.330,loss=1.33746839\n",
            "Number of Epochs 872:weights = 2.335,loss=1.33496833\n",
            "Number of Epochs 873:weights = 2.325,loss=1.33506370\n",
            "Number of Epochs 874:weights = 2.330,loss=1.33746815\n",
            "Number of Epochs 875:weights = 2.335,loss=1.33496809\n",
            "Number of Epochs 876:weights = 2.325,loss=1.33506393\n",
            "Number of Epochs 877:weights = 2.330,loss=1.33746791\n",
            "Number of Epochs 878:weights = 2.335,loss=1.33496785\n",
            "Number of Epochs 879:weights = 2.325,loss=1.33506417\n",
            "Number of Epochs 880:weights = 2.330,loss=1.33746791\n",
            "Number of Epochs 881:weights = 2.335,loss=1.33496785\n",
            "Number of Epochs 882:weights = 2.325,loss=1.33506441\n",
            "Number of Epochs 883:weights = 2.330,loss=1.33746791\n",
            "Number of Epochs 884:weights = 2.335,loss=1.33496785\n",
            "Number of Epochs 885:weights = 2.325,loss=1.33506465\n",
            "Number of Epochs 886:weights = 2.330,loss=1.33746767\n",
            "Number of Epochs 887:weights = 2.335,loss=1.33496761\n",
            "Number of Epochs 888:weights = 2.325,loss=1.33506489\n",
            "Number of Epochs 889:weights = 2.330,loss=1.33746743\n",
            "Number of Epochs 890:weights = 2.335,loss=1.33496737\n",
            "Number of Epochs 891:weights = 2.325,loss=1.33506513\n",
            "Number of Epochs 892:weights = 2.330,loss=1.33746743\n",
            "Number of Epochs 893:weights = 2.335,loss=1.33496737\n",
            "Number of Epochs 894:weights = 2.325,loss=1.33506536\n",
            "Number of Epochs 895:weights = 2.330,loss=1.33746743\n",
            "Number of Epochs 896:weights = 2.335,loss=1.33496737\n",
            "Number of Epochs 897:weights = 2.325,loss=1.33506560\n",
            "Number of Epochs 898:weights = 2.330,loss=1.33746719\n",
            "Number of Epochs 899:weights = 2.335,loss=1.33496714\n",
            "Number of Epochs 900:weights = 2.325,loss=1.33506584\n",
            "Number of Epochs 901:weights = 2.330,loss=1.33746696\n",
            "Number of Epochs 902:weights = 2.335,loss=1.33496690\n",
            "Number of Epochs 903:weights = 2.325,loss=1.33506608\n",
            "Number of Epochs 904:weights = 2.330,loss=1.33746696\n",
            "Number of Epochs 905:weights = 2.335,loss=1.33496690\n",
            "Number of Epochs 906:weights = 2.325,loss=1.33506632\n",
            "Number of Epochs 907:weights = 2.330,loss=1.33746696\n",
            "Number of Epochs 908:weights = 2.335,loss=1.33496690\n",
            "Number of Epochs 909:weights = 2.325,loss=1.33506656\n",
            "Number of Epochs 910:weights = 2.330,loss=1.33746672\n",
            "Number of Epochs 911:weights = 2.335,loss=1.33496666\n",
            "Number of Epochs 912:weights = 2.325,loss=1.33506680\n",
            "Number of Epochs 913:weights = 2.330,loss=1.33746648\n",
            "Number of Epochs 914:weights = 2.335,loss=1.33496642\n",
            "Number of Epochs 915:weights = 2.325,loss=1.33506703\n",
            "Number of Epochs 916:weights = 2.330,loss=1.33746648\n",
            "Number of Epochs 917:weights = 2.335,loss=1.33496642\n",
            "Number of Epochs 918:weights = 2.325,loss=1.33506727\n",
            "Number of Epochs 919:weights = 2.330,loss=1.33746648\n",
            "Number of Epochs 920:weights = 2.335,loss=1.33496642\n",
            "Number of Epochs 921:weights = 2.325,loss=1.33506751\n",
            "Number of Epochs 922:weights = 2.330,loss=1.33746624\n",
            "Number of Epochs 923:weights = 2.335,loss=1.33496618\n",
            "Number of Epochs 924:weights = 2.325,loss=1.33506775\n",
            "Number of Epochs 925:weights = 2.330,loss=1.33746600\n",
            "Number of Epochs 926:weights = 2.335,loss=1.33496594\n",
            "Number of Epochs 927:weights = 2.325,loss=1.33506799\n",
            "Number of Epochs 928:weights = 2.330,loss=1.33746600\n",
            "Number of Epochs 929:weights = 2.335,loss=1.33496594\n",
            "Number of Epochs 930:weights = 2.325,loss=1.33506823\n",
            "Number of Epochs 931:weights = 2.330,loss=1.33746600\n",
            "Number of Epochs 932:weights = 2.335,loss=1.33496594\n",
            "Number of Epochs 933:weights = 2.325,loss=1.33506846\n",
            "Number of Epochs 934:weights = 2.330,loss=1.33746576\n",
            "Number of Epochs 935:weights = 2.335,loss=1.33496571\n",
            "Number of Epochs 936:weights = 2.325,loss=1.33506870\n",
            "Number of Epochs 937:weights = 2.330,loss=1.33746552\n",
            "Number of Epochs 938:weights = 2.335,loss=1.33496547\n",
            "Number of Epochs 939:weights = 2.325,loss=1.33506894\n",
            "Number of Epochs 940:weights = 2.330,loss=1.33746552\n",
            "Number of Epochs 941:weights = 2.335,loss=1.33496547\n",
            "Number of Epochs 942:weights = 2.325,loss=1.33506918\n",
            "Number of Epochs 943:weights = 2.330,loss=1.33746552\n",
            "Number of Epochs 944:weights = 2.335,loss=1.33496547\n",
            "Number of Epochs 945:weights = 2.325,loss=1.33506942\n",
            "Number of Epochs 946:weights = 2.330,loss=1.33746529\n",
            "Number of Epochs 947:weights = 2.335,loss=1.33496523\n",
            "Number of Epochs 948:weights = 2.325,loss=1.33506966\n",
            "Number of Epochs 949:weights = 2.330,loss=1.33746505\n",
            "Number of Epochs 950:weights = 2.335,loss=1.33496499\n",
            "Number of Epochs 951:weights = 2.325,loss=1.33506989\n",
            "Number of Epochs 952:weights = 2.330,loss=1.33746505\n",
            "Number of Epochs 953:weights = 2.335,loss=1.33496499\n",
            "Number of Epochs 954:weights = 2.325,loss=1.33507013\n",
            "Number of Epochs 955:weights = 2.330,loss=1.33746505\n",
            "Number of Epochs 956:weights = 2.335,loss=1.33496499\n",
            "Number of Epochs 957:weights = 2.325,loss=1.33507037\n",
            "Number of Epochs 958:weights = 2.330,loss=1.33746481\n",
            "Number of Epochs 959:weights = 2.335,loss=1.33496475\n",
            "Number of Epochs 960:weights = 2.325,loss=1.33507061\n",
            "Number of Epochs 961:weights = 2.330,loss=1.33746457\n",
            "Number of Epochs 962:weights = 2.335,loss=1.33496451\n",
            "Number of Epochs 963:weights = 2.325,loss=1.33507085\n",
            "Number of Epochs 964:weights = 2.330,loss=1.33746457\n",
            "Number of Epochs 965:weights = 2.335,loss=1.33496451\n",
            "Number of Epochs 966:weights = 2.325,loss=1.33507109\n",
            "Number of Epochs 967:weights = 2.330,loss=1.33746457\n",
            "Number of Epochs 968:weights = 2.335,loss=1.33496451\n",
            "Number of Epochs 969:weights = 2.325,loss=1.33507133\n",
            "Number of Epochs 970:weights = 2.330,loss=1.33746433\n",
            "Number of Epochs 971:weights = 2.335,loss=1.33496428\n",
            "Number of Epochs 972:weights = 2.325,loss=1.33507156\n",
            "Number of Epochs 973:weights = 2.330,loss=1.33746409\n",
            "Number of Epochs 974:weights = 2.335,loss=1.33496404\n",
            "Number of Epochs 975:weights = 2.325,loss=1.33507180\n",
            "Number of Epochs 976:weights = 2.330,loss=1.33746409\n",
            "Number of Epochs 977:weights = 2.335,loss=1.33496404\n",
            "Number of Epochs 978:weights = 2.325,loss=1.33507204\n",
            "Number of Epochs 979:weights = 2.330,loss=1.33746409\n",
            "Number of Epochs 980:weights = 2.335,loss=1.33496404\n",
            "Number of Epochs 981:weights = 2.325,loss=1.33507228\n",
            "Number of Epochs 982:weights = 2.330,loss=1.33746386\n",
            "Number of Epochs 983:weights = 2.335,loss=1.33496380\n",
            "Number of Epochs 984:weights = 2.325,loss=1.33507252\n",
            "Number of Epochs 985:weights = 2.330,loss=1.33746362\n",
            "Number of Epochs 986:weights = 2.335,loss=1.33496356\n",
            "Number of Epochs 987:weights = 2.325,loss=1.33507276\n",
            "Number of Epochs 988:weights = 2.330,loss=1.33746362\n",
            "Number of Epochs 989:weights = 2.335,loss=1.33496356\n",
            "Number of Epochs 990:weights = 2.325,loss=1.33507299\n",
            "Number of Epochs 991:weights = 2.330,loss=1.33746362\n",
            "Number of Epochs 992:weights = 2.335,loss=1.33496356\n",
            "Number of Epochs 993:weights = 2.325,loss=1.33507323\n",
            "Number of Epochs 994:weights = 2.330,loss=1.33746338\n",
            "Number of Epochs 995:weights = 2.335,loss=1.33496332\n",
            "Number of Epochs 996:weights = 2.325,loss=1.33507347\n",
            "Number of Epochs 997:weights = 2.330,loss=1.33746314\n",
            "Number of Epochs 998:weights = 2.335,loss=1.33496308\n",
            "Number of Epochs 999:weights = 2.325,loss=1.33507371\n",
            "Number of Epochs 1000:weights = 2.330,loss=1.33746314\n",
            "Predicted value of F(10): 23.300739288330078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Design our model (input size, output size, forward pass)\n",
        "#Construct the loss and optimizer\n",
        "#training loop\n",
        "# - forward pass: compute prediciton\n",
        "#- backwward pass: gradients\n",
        "#- update our weights"
      ],
      "metadata": {
        "id": "tkGp-ez5zfIx"
      },
      "execution_count": 434,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "s35lyKBP7dw6"
      },
      "execution_count": 446,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([1,2,3,4,5],dtype=torch.float32)\n",
        "Y = torch.tensor([2,4,6,8,10],dtype=torch.float32)\n",
        "w = torch.tensor(0.0,dtype=torch.float32, requires_grad=True)"
      ],
      "metadata": {
        "id": "scC8COaj7gMD"
      },
      "execution_count": 452,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x):\n",
        "  return x*w"
      ],
      "metadata": {
        "id": "EVjNuXwx8EHN"
      },
      "execution_count": 453,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.01\n",
        "n_iters = 100"
      ],
      "metadata": {
        "id": "jIPx1tm-8IEb"
      },
      "execution_count": 454,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD([w],lr=lr)"
      ],
      "metadata": {
        "id": "ZQEsVhq28MZC"
      },
      "execution_count": 455,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epochs in range(n_iters):\n",
        "  y_pred = forward(X)\n",
        "  l = loss(Y,y_pred)\n",
        "  l.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  if epochs%1==0:\n",
        "    print(f'Number of Epochs {epochs+1}:weights = {w:.3f},loss={l:.8f}')\n",
        "print(f'Predicted value of F(10): {forward(10)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewSwDxne8biJ",
        "outputId": "6bb84b27-bd08-46b3-88da-8928310927d9"
      },
      "execution_count": 456,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epochs 1:weights = 0.440,loss=44.00000000\n",
            "Number of Epochs 2:weights = 0.783,loss=26.76960182\n",
            "Number of Epochs 3:weights = 1.051,loss=16.28662491\n",
            "Number of Epochs 4:weights = 1.260,loss=9.90878105\n",
            "Number of Epochs 5:weights = 1.423,loss=6.02850342\n",
            "Number of Epochs 6:weights = 1.550,loss=3.66774178\n",
            "Number of Epochs 7:weights = 1.649,loss=2.23145461\n",
            "Number of Epochs 8:weights = 1.726,loss=1.35761714\n",
            "Number of Epochs 9:weights = 1.786,loss=0.82597411\n",
            "Number of Epochs 10:weights = 1.833,loss=0.50252259\n",
            "Number of Epochs 11:weights = 1.870,loss=0.30573449\n",
            "Number of Epochs 12:weights = 1.899,loss=0.18600900\n",
            "Number of Epochs 13:weights = 1.921,loss=0.11316800\n",
            "Number of Epochs 14:weights = 1.938,loss=0.06885149\n",
            "Number of Epochs 15:weights = 1.952,loss=0.04188926\n",
            "Number of Epochs 16:weights = 1.962,loss=0.02548538\n",
            "Number of Epochs 17:weights = 1.971,loss=0.01550541\n",
            "Number of Epochs 18:weights = 1.977,loss=0.00943349\n",
            "Number of Epochs 19:weights = 1.982,loss=0.00573931\n",
            "Number of Epochs 20:weights = 1.986,loss=0.00349178\n",
            "Number of Epochs 21:weights = 1.989,loss=0.00212438\n",
            "Number of Epochs 22:weights = 1.992,loss=0.00129247\n",
            "Number of Epochs 23:weights = 1.993,loss=0.00078634\n",
            "Number of Epochs 24:weights = 1.995,loss=0.00047842\n",
            "Number of Epochs 25:weights = 1.996,loss=0.00029106\n",
            "Number of Epochs 26:weights = 1.997,loss=0.00017709\n",
            "Number of Epochs 27:weights = 1.998,loss=0.00010774\n",
            "Number of Epochs 28:weights = 1.998,loss=0.00006555\n",
            "Number of Epochs 29:weights = 1.999,loss=0.00003988\n",
            "Number of Epochs 30:weights = 1.999,loss=0.00002426\n",
            "Number of Epochs 31:weights = 1.999,loss=0.00001476\n",
            "Number of Epochs 32:weights = 1.999,loss=0.00000898\n",
            "Number of Epochs 33:weights = 1.999,loss=0.00000546\n",
            "Number of Epochs 34:weights = 2.000,loss=0.00000332\n",
            "Number of Epochs 35:weights = 2.000,loss=0.00000202\n",
            "Number of Epochs 36:weights = 2.000,loss=0.00000123\n",
            "Number of Epochs 37:weights = 2.000,loss=0.00000075\n",
            "Number of Epochs 38:weights = 2.000,loss=0.00000046\n",
            "Number of Epochs 39:weights = 2.000,loss=0.00000028\n",
            "Number of Epochs 40:weights = 2.000,loss=0.00000017\n",
            "Number of Epochs 41:weights = 2.000,loss=0.00000010\n",
            "Number of Epochs 42:weights = 2.000,loss=0.00000006\n",
            "Number of Epochs 43:weights = 2.000,loss=0.00000004\n",
            "Number of Epochs 44:weights = 2.000,loss=0.00000002\n",
            "Number of Epochs 45:weights = 2.000,loss=0.00000001\n",
            "Number of Epochs 46:weights = 2.000,loss=0.00000001\n",
            "Number of Epochs 47:weights = 2.000,loss=0.00000001\n",
            "Number of Epochs 48:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 49:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 50:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 51:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 52:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 53:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 54:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 55:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 56:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 57:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 58:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 59:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 60:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 61:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 62:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 63:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 64:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 65:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 66:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 67:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 68:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 69:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 70:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 71:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 72:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 73:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 74:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 75:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 76:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 77:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 78:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 79:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 80:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 81:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 82:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 83:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 84:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 85:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 86:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 87:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 88:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 89:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 90:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 91:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 92:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 93:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 94:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 95:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 96:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 97:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 98:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 99:weights = 2.000,loss=0.00000000\n",
            "Number of Epochs 100:weights = 2.000,loss=0.00000000\n",
            "Predicted value of F(10): 19.999998092651367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch"
      ],
      "metadata": {
        "id": "nbXGhjmW9JLj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([1,2,3,4,5,6],dtype =torch.float32)\n",
        "Y = torch.tensor([2,4,6,8,10,12],dtype=torch.float32)\n",
        "w = torch.tensor(0.0, dtype=torch.float32,requires_grad=True)"
      ],
      "metadata": {
        "id": "us8o5T7-9vMa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x):\n",
        "  return w*x"
      ],
      "metadata": {
        "id": "xps45qD5-GlP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD([w],lr=0.01)"
      ],
      "metadata": {
        "id": "BKaIfQpx-K_j"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epochs in range(100):\n",
        "  y_pred = forward(X)\n",
        "  l = loss(Y,y_pred)\n",
        "  l.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  print(f'Predict F(10): {forward(10)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09DmhPOX-dGU",
        "outputId": "18880d84-7119-4cd8-ab3b-5af99b469f2d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict F(10): 6.066666603088379\n",
            "Predict F(10): 10.293110847473145\n",
            "Predict F(10): 13.237533569335938\n",
            "Predict F(10): 15.28881549835205\n",
            "Predict F(10): 16.71787452697754\n",
            "Predict F(10): 17.71345329284668\n",
            "Predict F(10): 18.40703773498535\n",
            "Predict F(10): 18.890235900878906\n",
            "Predict F(10): 19.226863861083984\n",
            "Predict F(10): 19.461381912231445\n",
            "Predict F(10): 19.62476348876953\n",
            "Predict F(10): 19.73858642578125\n",
            "Predict F(10): 19.817882537841797\n",
            "Predict F(10): 19.873125076293945\n",
            "Predict F(10): 19.911609649658203\n",
            "Predict F(10): 19.93842124938965\n",
            "Predict F(10): 19.95709991455078\n",
            "Predict F(10): 19.97011375427246\n",
            "Predict F(10): 19.97917938232422\n",
            "Predict F(10): 19.98549461364746\n",
            "Predict F(10): 19.98989486694336\n",
            "Predict F(10): 19.99295997619629\n",
            "Predict F(10): 19.99509620666504\n",
            "Predict F(10): 19.996583938598633\n",
            "Predict F(10): 19.99761962890625\n",
            "Predict F(10): 19.998342514038086\n",
            "Predict F(10): 19.998844146728516\n",
            "Predict F(10): 19.999195098876953\n",
            "Predict F(10): 19.999439239501953\n",
            "Predict F(10): 19.999610900878906\n",
            "Predict F(10): 19.99972915649414\n",
            "Predict F(10): 19.99981117248535\n",
            "Predict F(10): 19.999868392944336\n",
            "Predict F(10): 19.999908447265625\n",
            "Predict F(10): 19.999935150146484\n",
            "Predict F(10): 19.999954223632812\n",
            "Predict F(10): 19.999967575073242\n",
            "Predict F(10): 19.999977111816406\n",
            "Predict F(10): 19.999984741210938\n",
            "Predict F(10): 19.999988555908203\n",
            "Predict F(10): 19.99999237060547\n",
            "Predict F(10): 19.999996185302734\n",
            "Predict F(10): 19.999996185302734\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n",
            "Predict F(10): 19.999998092651367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using model instead of forward pass\n",
        "X = torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
        "Y = torch.tensor([[2],[4],[6],[8]],dtype=torch.float32)\n",
        "X_test = torch.tensor([5],dtype=torch.float32)"
      ],
      "metadata": {
        "id": "PeHaXD06_EFC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples,n_features = X.shape"
      ],
      "metadata": {
        "id": "4h9NLwoNBz0Q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size=n_features\n",
        "output_size = n_features"
      ],
      "metadata": {
        "id": "dbtsNKAHCHpn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the  model\n",
        "model = LinearRegressionModel(input_size,output_size)"
      ],
      "metadata": {
        "id": "zhKMpLvw_1TU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Prediction before training: f(5)={model(X_test).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxAtK0WiCiax",
        "outputId": "5acacac2-29ab-41a2-e337-5033e3eaad70"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5)=-4.150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)"
      ],
      "metadata": {
        "id": "lTka04AdC78Q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epochs in range(100):\n",
        "  y_pred = model(X)\n",
        "  l = loss(Y,y_pred)\n",
        "  l.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epochs%1==0:\n",
        "    [w,b] = model.parameters()\n",
        "    print(f'epoch {epochs+1}:w {w[0][0].item():.3f},loss={l:.8f}')\n",
        "print(f'prediction after training: f(5)={model(X_test).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8Me3KsuDM90",
        "outputId": "1ca7b102-06b1-47f5-d35f-5156cb483346"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1:w -0.290,loss=63.11489105\n",
            "epoch 2:w 0.072,loss=43.80061340\n",
            "epoch 3:w 0.374,loss=30.39880753\n",
            "epoch 4:w 0.626,loss=21.09953880\n",
            "epoch 5:w 0.835,loss=14.64693260\n",
            "epoch 6:w 1.010,loss=10.16956902\n",
            "epoch 7:w 1.156,loss=7.06278229\n",
            "epoch 8:w 1.277,loss=4.90701199\n",
            "epoch 9:w 1.378,loss=3.41113043\n",
            "epoch 10:w 1.462,loss=2.37313128\n",
            "epoch 11:w 1.533,loss=1.65284991\n",
            "epoch 12:w 1.591,loss=1.15302372\n",
            "epoch 13:w 1.640,loss=0.80616868\n",
            "epoch 14:w 1.681,loss=0.56545669\n",
            "epoch 15:w 1.715,loss=0.39839563\n",
            "epoch 16:w 1.743,loss=0.28243935\n",
            "epoch 17:w 1.767,loss=0.20194358\n",
            "epoch 18:w 1.787,loss=0.14605384\n",
            "epoch 19:w 1.803,loss=0.10723782\n",
            "epoch 20:w 1.817,loss=0.08026902\n",
            "epoch 21:w 1.829,loss=0.06152068\n",
            "epoch 22:w 1.838,loss=0.04847685\n",
            "epoch 23:w 1.847,loss=0.03939134\n",
            "epoch 24:w 1.853,loss=0.03305275\n",
            "epoch 25:w 1.859,loss=0.02862032\n",
            "epoch 26:w 1.864,loss=0.02551072\n",
            "epoch 27:w 1.868,loss=0.02331931\n",
            "epoch 28:w 1.872,loss=0.02176513\n",
            "epoch 29:w 1.874,loss=0.02065333\n",
            "epoch 30:w 1.877,loss=0.01984867\n",
            "epoch 31:w 1.879,loss=0.01925736\n",
            "epoch 32:w 1.881,loss=0.01881428\n",
            "epoch 33:w 1.883,loss=0.01847425\n",
            "epoch 34:w 1.884,loss=0.01820589\n",
            "epoch 35:w 1.885,loss=0.01798747\n",
            "epoch 36:w 1.886,loss=0.01780390\n",
            "epoch 37:w 1.887,loss=0.01764469\n",
            "epoch 38:w 1.888,loss=0.01750260\n",
            "epoch 39:w 1.889,loss=0.01737257\n",
            "epoch 40:w 1.889,loss=0.01725108\n",
            "epoch 41:w 1.890,loss=0.01713571\n",
            "epoch 42:w 1.891,loss=0.01702477\n",
            "epoch 43:w 1.891,loss=0.01691707\n",
            "epoch 44:w 1.892,loss=0.01681184\n",
            "epoch 45:w 1.892,loss=0.01670849\n",
            "epoch 46:w 1.893,loss=0.01660663\n",
            "epoch 47:w 1.893,loss=0.01650597\n",
            "epoch 48:w 1.893,loss=0.01640636\n",
            "epoch 49:w 1.894,loss=0.01630761\n",
            "epoch 50:w 1.894,loss=0.01620964\n",
            "epoch 51:w 1.894,loss=0.01611241\n",
            "epoch 52:w 1.895,loss=0.01601584\n",
            "epoch 53:w 1.895,loss=0.01591996\n",
            "epoch 54:w 1.895,loss=0.01582468\n",
            "epoch 55:w 1.896,loss=0.01572997\n",
            "epoch 56:w 1.896,loss=0.01563588\n",
            "epoch 57:w 1.896,loss=0.01554237\n",
            "epoch 58:w 1.897,loss=0.01544941\n",
            "epoch 59:w 1.897,loss=0.01535701\n",
            "epoch 60:w 1.897,loss=0.01526520\n",
            "epoch 61:w 1.898,loss=0.01517391\n",
            "epoch 62:w 1.898,loss=0.01508319\n",
            "epoch 63:w 1.898,loss=0.01499300\n",
            "epoch 64:w 1.899,loss=0.01490335\n",
            "epoch 65:w 1.899,loss=0.01481427\n",
            "epoch 66:w 1.899,loss=0.01472569\n",
            "epoch 67:w 1.900,loss=0.01463763\n",
            "epoch 68:w 1.900,loss=0.01455011\n",
            "epoch 69:w 1.900,loss=0.01446311\n",
            "epoch 70:w 1.901,loss=0.01437666\n",
            "epoch 71:w 1.901,loss=0.01429071\n",
            "epoch 72:w 1.901,loss=0.01420525\n",
            "epoch 73:w 1.901,loss=0.01412034\n",
            "epoch 74:w 1.902,loss=0.01403589\n",
            "epoch 75:w 1.902,loss=0.01395198\n",
            "epoch 76:w 1.902,loss=0.01386856\n",
            "epoch 77:w 1.903,loss=0.01378563\n",
            "epoch 78:w 1.903,loss=0.01370322\n",
            "epoch 79:w 1.903,loss=0.01362130\n",
            "epoch 80:w 1.903,loss=0.01353986\n",
            "epoch 81:w 1.904,loss=0.01345891\n",
            "epoch 82:w 1.904,loss=0.01337842\n",
            "epoch 83:w 1.904,loss=0.01329843\n",
            "epoch 84:w 1.905,loss=0.01321895\n",
            "epoch 85:w 1.905,loss=0.01313989\n",
            "epoch 86:w 1.905,loss=0.01306133\n",
            "epoch 87:w 1.905,loss=0.01298323\n",
            "epoch 88:w 1.906,loss=0.01290563\n",
            "epoch 89:w 1.906,loss=0.01282846\n",
            "epoch 90:w 1.906,loss=0.01275177\n",
            "epoch 91:w 1.907,loss=0.01267554\n",
            "epoch 92:w 1.907,loss=0.01259974\n",
            "epoch 93:w 1.907,loss=0.01252439\n",
            "epoch 94:w 1.907,loss=0.01244955\n",
            "epoch 95:w 1.908,loss=0.01237511\n",
            "epoch 96:w 1.908,loss=0.01230109\n",
            "epoch 97:w 1.908,loss=0.01222757\n",
            "epoch 98:w 1.909,loss=0.01215446\n",
            "epoch 99:w 1.909,loss=0.01208178\n",
            "epoch 100:w 1.909,loss=0.01200953\n",
            "prediction after training: f(5)=9.813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self,input_dim,output_dim):\n",
        "    super(LinearRegression,self).__init__()\n",
        "\n",
        "    self.lin = nn.Linear(input_dim,output_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.lin(x)\n"
      ],
      "metadata": {
        "id": "oSuAvROZEQWa"
      },
      "execution_count": 489,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegressionModel(nn.Module):\n",
        "  def __init__(self,input_dims,output_dims):\n",
        "    super(LinearRegressionModel,self).__init__()\n",
        "\n",
        "    self.linear_module = nn.Linear(input_dims,output_dims)\n",
        "                                   \n",
        "  def forward(self,x):\n",
        "    return self.linear_module(x)"
      ],
      "metadata": {
        "id": "-Xg4y4zrHmQt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Uc6ZEkL-hUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EJt85cVO9Bg0"
      }
    }
  ]
}